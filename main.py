import os

#ì„¤ì¹˜ ì™„ë£Œí›„ í™˜ê²½ì„¤ì •
# python íŒ¨í‚¤ì§€ë¡œ JAVA_HOME ì„¤ì •í•˜ê¸°
os.environ["JAVA_HOME"] = "/opt/conda"

# í•„ìš” íŒ¨í‚¤ì§€ ì¶”ê°€
import time
import datetime
import pickle
import docx
from docx import Document
import re
import sys, io
import tempfile
import shutil
import openpyxl
from openpyxl.utils import get_column_letter
from openpyxl.styles import Font, Border, Side, PatternFill, Alignment, Protection, GradientFill, Color
from openpyxl.cell import MergedCell
from openpyxl import load_workbook
from docx.shared import Pt, Cm
from docx.enum.text import WD_PARAGRAPH_ALIGNMENT
from docx.enum.section import WD_SECTION

# í•œê¸€ í† í°í™” konlpy ëª¨ë“ˆ
# from konlpy.tag import Okt
from ckonlpy.tag import Twitter
# okt = Okt()
twitter = Twitter() # twitterê°€ oktë³´ë‹¤ ì„±ëŠ¥ ë†’ìŒ

# from gensim.models import Word2Vec

# ë¬¸ì„œì‘ì—… ì¤‘ í•„ìš” ëª¨ë“ˆ
import re
import glob
import warnings
# import gensim

# model checkpoint
import mygsmod as gs
from mygsmod import ModelCheckpoint

import pandas as pd
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import seaborn as sns
import tempfile
from PIL import Image

import streamlit as st

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import confusion_matrix

import torch.nn as nn
import torch
import torch.optim as optim
import torchsummary
from torchsummary import summary
from torch.nn import functional as F
from torch.utils.data import Dataset, DataLoader
from torchtext.vocab import build_vocab_from_iterator
from torch.nn.utils.rnn import pad_sequence

# ì§ì ‘ ì €ì¥í•´ë‘” ëª¨ë“ˆ í˜¸ì¶œ
import ta_eda_modeling
from ta_eda_modeling import word_map, noun_map, cr_word_map, stopwords
import ta_auto
from ta_auto import clear_directory, main_grouping, save_to_excel, categorize_sheet, group_and_save_data, drop_first_row, insert_row_based_on_condition, worklist_type_transform, split_excel_sheets_to_files, copy_style, cdv_checklist, hex_checklist, afc_checklist,integrate_docx_files, create_handbook_hex_afc, create_handbook_cdv, afc_checklist_merge,hex_checklist_merge,  cdv_checklist_merge

# í•œê¸€ê¹¨ì§ ë°©ì§€ì½”ë“œ 
font_location = '/home/sagemaker-user/gsc/NanumGothic.ttf'
fm.fontManager.addfont(font_location)
font_name = fm.FontProperties(fname=font_location).get_name()
matplotlib.rc('font', family=font_name)
matplotlib.rc('axes', unicode_minus=False)

# ì›¹ í˜ì´ì§€ ê¸°ë³¸ ì„¤ì •
# page title: ë°ì´í„° ë¶„ì„ ë° ëª¨ë¸ë§ ëŒ€ì‹œë³´ë“œ
st.set_page_config(
    page_title="TA Worklist & Checklist Automation", # page íƒ€ì´í‹€
    page_icon="ğŸ§Š", # page ì•„ì´ì½˜
    layout="wide", # wide, centered
    initial_sidebar_state="auto", # ì‚¬ì´ë“œ ë°” ì´ˆê¸° ìƒíƒœ
    menu_items={
        'Get Help': 'https://streamlit.io',
        'Report a bug': None,
        'About': '2023 GS CDS Class',
    }
)

# ì‹¤ìŠµ ì†Œê°œ í˜ì´ì§€ ì¶œë ¥ í•¨ìˆ˜
# ì†Œê°œ í˜ì´ì§€ëŠ” ê¸°ë³¸ìœ¼ë¡œ ì œê³µë©ë‹ˆë‹¤.
def front_page():
    st.title('TA Worklist & Checklist Automation Tool')
    st.header('Toolì˜ ê¸°ëŠ¥ ì†Œê°œ')
    st.write('1. TA Reportì—ì„œ Worklist ì‘ì„± ì‚¬í•­ ìë™ ì¶”ì¶œ')
    st.write('2. ì‘ì„±ëœ Worklistë¡œë¶€í„° Checklist ìë™ ìƒì„±')
    st.write('3. Checklistë¡œë¶€í„° TA Handbook ìë™ ìƒì„±')
    st.markdown(' 1. EDA í˜ì´ì§€ ìƒì„±')
    st.markdown('''
        - íŒŒì¼ ì—…ë¡œë“œ (TA Report Word íŒŒì¼ ì—…ë¡œë“œ)
        - íŒŒì¼ í˜•ì‹ ë³€ê²½ (Word íŒŒì¼ -> txt íŒŒì¼ -> DataFrame)
        - ë°ì´í„° ì „ì²˜ë¦¬ (ë¬¸ì¥ ë¶„ë¦¬, ë‹¨ì–´ mapping, í•œê¸€í™”, ë¹ˆí–‰ ì œê±°, í† í°í™”, ë¶ˆìš©ì–´ ì œê±°)
        - ì¶”ê°€ ì „ì²˜ë¦¬ for ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸(CountVectorizer í•¨ìˆ˜ë¥¼ í†µí•œ ë²¡í„°í™”)
        - ì¶”ê°€ ì „ì²˜ë¦¬ for ë”¥ëŸ¬ë‹ ëª¨ë¸(Customized Twitter í•¨ìˆ˜ë¥¼ í†µí•œ í† í°í™”)
    ''')
    st.markdown(' 2. Modeling í˜ì´ì§€ ìƒì„±')
    st.markdown('''
        - ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ì‚¬ìš©ì„ ìœ„í•œ ë°ì´í„° ë¶„í• 
        - ë”¥ëŸ¬ë‹ ëª¨ë¸ ì‚¬ìš©ì„ ìœ„í•œ ë°ì´í„° ë¶„í• 
        - ëª¨ë¸ë§ (í•˜ì´í¼ íŒŒë¼ë¯¸í„° ì„¤ì •)
        - ëª¨ë¸ë§ ê²°ê³¼ í™•ì¸ (í‰ê°€ ì¸¡ë„, Confusion Matrix)
    ''')
    st.markdown(' 3. Model Using í˜ì´ì§€ ìƒì„±')
    st.markdown('''
        - ì…ë ¥ ê°’ ì„¤ì • (ë©”ë‰´)
        - ì¶”ë¡  
    ''')    
    
#-----------------------------------------------------------------------------------------
# customized_twitterì— íŠ¹ì • ëª…ì‚¬ ì…ë ¥
nouns = noun_map.values()
twitter.add_dictionary(nouns, 'Noun')

# ë¶ˆìš©ì–´ ì‚¬ì „ì— ì¶”ê°€
plus_stopwords = ['ë„', 'ëŠ”', 'ë‹¤', 'ì˜', 'ê°€', 'ì´','ì€', 'í•œ', 'ì—', 'í•˜', 'ê³ ', 'ì„','ë¥¼', 'ì¸', 'ë“¯', 'ê³¼', 'ì™€', 'ë„¤',
             'ë“¤', 'ë“¯', 'ì§€', 'ì„', 'ê²Œ', 'ê¸°', 'ê°œ', 'ê°œì†Œ', 'ë…„', 'ë²ˆ'
]
for word in plus_stopwords:
    stopwords.append(word)

# ë¶€ì‹ìœ¨ ì¸ì‹ ìœ„í•œ ì‚¬ì „
cr_word_map.update({round(i*0.001, 2): 'ë†’ìŒ' for i in range(101, 1000)})
cr_word_map[0.0] = 'ë‚®ìŒ'
cr_word_map[0.00] = 'ë‚®ìŒ'
#-----------------------------------------------------------------------------------------------------    
warnings.filterwarnings('ignore')  # ê²½ê³  ë©”ì‹œì§€ ë¬´ì‹œ
pd.set_option('display.max_colwidth', 700)  # pandasì—ì„œ í‘œì‹œí•˜ëŠ” ìµœëŒ€ ì—´ ë„ˆë¹„ ì„¤ì •
#-----------------------------------------------------------------------------------------------------
# GPU ì‚¬ìš©
device = 'cuda' if torch.cuda.is_available() else 'cpu'

# ì‹œë“œ ì„¤ì •
torch.manual_seed(777)
torch.cuda.manual_seed_all(777)
#----------------------------------------------------------------------------------------------------
# 1. file load í•¨ìˆ˜
# 2. íŒŒì¼ í™•ì¥ìì— ë§ê²Œ ì½ì–´ì„œ dfìœ¼ë¡œ ë¦¬í„´í•˜ëŠ” í•¨ìˆ˜
# 3. ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´ ìºì‹± ê¸°ëŠ¥ ì´ìš©
@st.cache_data
def load_file(file):
    
    # í™•ì¥ì ë¶„ë¦¬
    ext = file.name.split('.')[-1]
    
    # í™•ì¥ì ë³„ ë¡œë“œ í•¨ìˆ˜ êµ¬ë¶„
    if ext == 'docx':
        return pd.read_docx(file)
    
# word -> txt í•¨ìˆ˜
def add_requirement_to_sentences(text):
    # Split the text based on the pattern "(number) "
    sentences = re.split(r'(\d+\.\s|\(\d+\)\s)', text)
    
    # If no "(number) " pattern is found, treat the entire text as one sentence
    if len(sentences) == 1:
        return text.strip() + ' í•„ìš”.'

    # The split function will capture the delimiters as well, so we'll have to merge them back into the sentences
    processed_sentences = []
    for i in range(1, len(sentences), 2):
        sentence = sentences[i] + sentences[i+1].strip()
        if sentence.endswith('.'):
            sentence = sentence[:-1]
        sentence += ' í•„ìš”.'
        processed_sentences.append(sentence)
    
    # Join the sentences back together
    processed_text = ' '.join(processed_sentences)
    
    return processed_text

# ë¬¸ì„œë¥¼ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜
def process_document_txt(input_filepath, output_directory):
    if not os.path.exists(output_directory):
        os.makedirs(output_directory)

    doc = Document(input_filepath)

    capture_next_table = False
    previous_paragraph = None
    filepath = ""

    for element in doc.element.body:
        if element.tag.endswith('p'):
            paragraph = element.text
            if paragraph:
                if '1. ì¥ì¹˜ ê¸°ë³¸ ì •ë³´' in paragraph and previous_paragraph is not None:
                    device_info = previous_paragraph
                    filename = "".join(x for x in device_info if x.isalnum() or x in " _-").rstrip()
                    filename += ".txt"
                    filepath = os.path.join(output_directory, filename)
                    with open(filepath, 'w', encoding='utf-8') as txt_file:
                        txt_file.write(previous_paragraph + '\n')

                if '2. ê°œë°©ê²€ì‚¬ ê²°ê³¼' in paragraph:
                    capture_next_table = True

                previous_paragraph = paragraph

        elif element.tag.endswith('tbl') and capture_next_table:
            table = None
            for tbl in doc.tables:
                if tbl._element == element:
                    table = tbl
                    break

            if table and filepath:  # Ensure filepath is not empty
                with open(filepath, 'a', encoding='utf-8') as txt_file:
                    for row in table.rows[1:]:  # Skip the header row
                        first_column_text = row.cells[0].text.strip()
                        second_column_text = row.cells[1].text.strip()
                        # Apply special processing if the first column contains 'ì°¨ê¸° TA Recommend'
                        if first_column_text == 'ì°¨ê¸° TA Recommend':
                            if second_column_text not in ('N/A', ''):
                                second_column_text = add_requirement_to_sentences(second_column_text)
                        # Write the second column text to the file regardless of the first column content
                        txt_file.write(second_column_text + '\n')
                capture_next_table = False

        elif capture_next_table and ('3. ì°¸ê³  ì‚¬ì§„' in paragraph or '3. ì‚¬ì§„' in paragraph):
            capture_next_table = False
            
# txt -> df í•¨ìˆ˜
def custom_sentence_splitter(text):
    # 'ìˆ«ì' + '.' + 'ê³µë°±' or 'ìˆ«ì' + '.' + 'ë¬¸ì' ë¬¸ìë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìš°ì„ ì ìœ¼ë¡œ ë¶„ë¦¬
    primary_sentences = re.split(r'(?<=\d)\.\s|(?<=\d)\.(?=[a-zA-Z\uAC00-\uD7A3])', text)
    
    refined_sentences = []
    for sent in primary_sentences:
        # ì¶”ê°€ ë¶„ë¦¬: 'ìˆ«ì' + ')' + 'ê³µë°±'
        if re.search(r'\d\)\s', sent):
            parts = re.split(r'(?<=\d\))\s', sent)
            refined_sentences.extend(parts)
        # ì¶”ê°€ ë¶„ë¦¬: 'í•œê¸€ ë¬¸ì ë’¤ì— ì˜¤ëŠ” ë§ˆì¹¨í‘œ(.) + 'ê³µë°±'
        elif re.search(r'[\uAC00-\uD7A3]\.\s', sent):
            parts = re.split(r'(?<=[\uAC00-\uD7A3])\.\s', sent)
            refined_sentences.extend(s + '.' for s in parts if s)  # ë§ˆì¹¨í‘œ ì¶”ê°€
        # ì¶”ê°€ ë¶„ë¦¬: 'í•œê¸€ ë¬¸ì ë’¤ì— ê³µë°± 2ì¹¸ ì´ìƒì¼ ê²½ìš° ë¶„ë¦¬'
        elif re.search(r'[\uAC00-\uD7A3]\s\s', sent):
            parts = re.split(r'(?<=[\uAC00-\uD7A3])\s\s', sent)
            refined_sentences.extend(s + '.' for s in parts if s)
        # ì¶”ê°€ ë¶„ë¦¬: '('+'ìˆ«ì'+')' ë¶„ë¦¬'
        elif re.search(r'(\(\d+\)\s)', sent):
            parts = re.split(r'(?<=(\(\d+\)\s)', sent)
            refined_sentences.extend(s + '.' for s in parts if s) 
        else:
            refined_sentences.append(sent)
            
    # ì„ í–‰ í˜¹ì€ í›„í–‰ ê³µë°±ê³¼ ë¹ˆ ë¬¸ìì—´ ì œê±°
    refined_sentences = [sent.strip() for sent in refined_sentences if sent.strip()]
    return refined_sentences

def process_text_files(path):
    all_files = glob.glob(os.path.join(path, '*.txt'))
    filename_list = []
    sent_list = []

    for file_ in all_files:
        with open(file_, 'r', encoding='utf-8') as f:
            first_line = f.readline().strip()
            remaining_text = f.read().strip()

        sentences = custom_sentence_splitter(remaining_text)
        sentences = [re.sub(r'([a-zA-Z])([\uAC00-\uD7A3])', r'\1 \2', sent) for sent in sentences]

        for sent in sentences:
            filename_list.append(first_line)
            sent_list.append(sent)

    return pd.DataFrame({'filename': filename_list, 'sent_text': sent_list})

# '.' ê¸°ì¤€ ë¬¸ì¥ ì¶”ê°€ ë¶„ë¦¬
def split_sentences(text):
    # Divide the text into sentences based on the period (.) following the Hangul characters
    sentences = re.split('(?<=[\uAC00-\uD7A3])\.', text)
    sentences = [sent.strip() for sent in sentences if sent]  # Select only non-space sentences and remove leading and trailing spaces
    
    # Insert a space between English and Korean characters
    sentences = [re.sub(r'([a-zA-Z])([\uAC00-\uD7A3])', r'\1 \2', sent) for sent in sentences]
    return sentences

# ê´„í˜¸ ì•ˆì˜ ë¬¸ì ì²˜ë¦¬
def brackets_clean(text):
            # ê´„í˜¸ ì•ˆì˜ ìˆ«ì, íŠ¹ìˆ˜ê¸°í˜¸ë§Œ ì œê±°
            clean1 = re.sub(r'\(([\d\W_]*?)\)', '()', text)
    
            # ê´„í˜¸ì™€ ë¬¸ìê°„ ë„ì–´ì“°ê¸°
            clean2 = re.sub(r'([^\s])(\()', r'\1 \2', clean1)
            clean3 = re.sub(r'(\))([^\s])', r'\1 \2', clean2)
    
            return clean3

# ë‹¨ì–´ ë³€í™˜ 1 : word_mapì—ì„œ ë‹¨ì–´ ê¸¸ì´ê°€ ê¸´ ìˆœìœ¼ë¡œ ë¨¼ì € ë³€í™˜ ì‹¤ì‹œ, re-tubingì€ ë¦¬íŠœë¹™ìœ¼ë¡œ, tubingì€ íŠœë¹™ìœ¼ë¡œ ì¸ì‹ë˜ë„ë¡ í•¨
def replace(match):
    return word_map[match.group(0)]

def apply_replacement1(text):
    # word_mapì˜ í‚¤ë¥¼ ê¸¸ì´ì— ë”°ë¼ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬í•©ë‹ˆë‹¤.
    sorted_keys = sorted(word_map.keys(), key=len, reverse=True)
    # lookbehindì™€ lookaheadë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¨ì–´ì˜ ì¼ë¶€ë§Œ ë§¤ì¹˜ë˜ë„ë¡ íŒ¨í„´ì„ ìˆ˜ì •í•©ë‹ˆë‹¤.
    pattern = re.compile('|'.join('(?<!\w){}(?!\w)'.format(re.escape(k)) for k in sorted_keys),re.IGNORECASE)
    return pattern.sub(replace, text)

# Function to apply the replacement within the text
def apply_replacement2(text):
    # Pattern that matches the words to be replaced even if they are part of a larger word
    pattern = re.compile('|'.join(map(re.escape, word_map.keys())))
    return pattern.sub(replace, text)

# ê´„í˜¸ ì•ˆì˜ ë¬¸ì ì²˜ë¦¬
def brackets_clean(text):
            # ê´„í˜¸ ì•ˆì˜ ìˆ«ì, íŠ¹ìˆ˜ê¸°í˜¸ë§Œ ì œê±°
            clean1 = re.sub(r'\(([\d\W_]*?)\)', '()', text)
    
            # ê´„í˜¸ì™€ ë¬¸ìê°„ ë„ì–´ì“°ê¸°
            clean2 = re.sub(r'([^\s])(\()', r'\1 \2', clean1)
            clean3 = re.sub(r'(\))([^\s])', r'\1 \2', clean2)
    
            return clean3

# default ì°¾ê¸° ë¶€ë¶„ ë‹¨ì–´ ë³€í™˜ 1 : word_mapì—ì„œ ë‹¨ì–´ ê¸¸ì´ê°€ ê¸´ ìˆœìœ¼ë¡œ ë¨¼ì € ë³€í™˜ ì‹¤ì‹œ, re-tubingì€ ë¦¬íŠœë¹™ìœ¼ë¡œ, tubingì€ íŠœë¹™ìœ¼ë¡œ ì¸ì‹ë˜ë„ë¡ í•¨
def replace_default(match):
    return word_map.get(match.group(0), match.group(0))

def apply_replacement1_default(text):
    # word_mapì˜ í‚¤ë¥¼ ê¸¸ì´ì— ë”°ë¼ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬í•©ë‹ˆë‹¤.
    sorted_keys = sorted(word_map.keys(), key=len, reverse=True)
    # lookbehindì™€ lookaheadë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¨ì–´ì˜ ì¼ë¶€ë§Œ ë§¤ì¹˜ë˜ë„ë¡ íŒ¨í„´ì„ ìˆ˜ì •í•©ë‹ˆë‹¤.
    pattern = re.compile('|'.join('(?<!\w){}(?!\w)'.format(re.escape(k)) for k in sorted_keys),re.IGNORECASE)
    return pattern.sub(replace_default, text)

# default ì°¾ê¸° ë¶€ë¶„ ë‹¨ì–´ ë³€í™˜ 2
def apply_replacement2_default(text):
    # Pattern that matches the words to be replaced even if they are part of a larger word
    pattern = re.compile('|'.join(map(re.escape, word_map.keys())))
    return pattern.sub(replace_default, text)

# ë¶€ì‹ìœ¨ ì¸ì‹ ì²˜ë¦¬ í•¨ìˆ˜
def replace_with_words(text, word_map):
    # Define a regular expression pattern for the intended formats
    pattern = r'(\d+\.\d{2})\s*mm(?:/yr|/year)?|(\d+\.\d{2})\*mm(?:/yr|/year)?'

    def replace(match):
        # Extract the number and round it
        num = round(float(match.group(1)), 2)
        # Replace with corresponding word if exists
        return f'{word_map.get(num, num)} mm/yr'  # Default to original number if not in word map

    # Replace all occurrences in the text
    return re.sub(pattern, replace, text)

# í•œê¸€ ì¶”ì¶œ í•¨ìˆ˜
def extract_korean(text):
    hangul = re.compile('[^ ã„±-ã…£ ê°€-í£]')  
    result = hangul.sub('', text)
    
    return result

# ë¨¸ì‹ ëŸ¬ë‹ ì ìš© ìœ„í•œ í† í°í™” í•¨ìˆ˜
def tokenize(doc):
    # pos ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ í† í°í™” ë° í’ˆì‚¬ íƒœê¹…, ì •ê·œí™” ë° ê¸°ë³¸í˜• ë³€í™˜ ìˆ˜í–‰
    return [word for word, tag in twitter.pos(doc, norm=True, stem=True)]

# ë”¥ëŸ¬ë‹ ì ìš© ìœ„í•œ í† í°í™” í•¨ìˆ˜
def tokenize1(doc):
    # pos ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ í† í°í™” ë° í’ˆì‚¬ íƒœê¹…, ì •ê·œí™” ë° ê¸°ë³¸í˜• ë³€í™˜ ìˆ˜í–‰
    return [word for word, tag in twitter.pos(doc, norm=True, stem=False)]

# file uploader 
# session_stateì— ë‹¤ìŒê³¼ ê°™ì€ 3ê°œ ê°’ì„ ì €ì¥í•˜ì—¬ ê´€ë¦¬í•¨
# 1. st.session_state['eda_state'] = {}
#  1.1 : st.session_state['eda_state']['current_file']  / st.session_state['eda_state']['current_data']
# 2. st.session_state['modeling_state'] = {}
# 3. st.session_state['using_state'] = {}
def file_uploader():
    # íŒŒì¼ ì—…ë¡œë” ìœ„ì ¯ ì¶”ê°€ (Word ë¬¸ì„œ ì„ íƒ)
    file = st.file_uploader("Select file (Word document)", type=['docx'])
    
    if file is not None:
        with st.spinner('ë³€í™˜ ì‘ì—… ì¤‘...'):
            # ìƒˆ íŒŒì¼ ì—…ë¡œë“œ ì‹œ ê¸°ì¡´ ìƒíƒœ ì´ˆê¸°í™”
            st.session_state['eda_state'] = {}
            st.session_state['modeling_state'] = {}
            st.session_state['using_state'] = {}

            # ì—…ë¡œë“œëœ íŒŒì¼ì„ ì„ì‹œ ì €ì¥
            temp_file_path = "temp_uploaded_file.docx"
            with open(temp_file_path, "wb") as f:
                f.write(file.getbuffer())

            # word -> txt
            output_directory = "Word to Text File"
            process_document_txt(temp_file_path, output_directory)  # ìˆ˜ì •ëœ í•¨ìˆ˜ ì´ë¦„

            # ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
            st.session_state['eda_state']['current_file'] = file
            st.success(f"**:blue[{output_directory} ë³€í™˜ ì™„ë£Œ!!]**")

            # txt -> df
            df = process_text_files(output_directory)  # ìˆ˜ì •ëœ í•¨ìˆ˜ ì´ë¦„
            st.session_state['eda_state']['current_data'] = df
            st.success("Text to DataFrame ë³€í™˜ ì™„ë£Œ!!")

        # ìƒˆë¡œ ì—…ë¡œë“œëœ íŒŒì¼ ë¡œë“œ
        if 'current_data' in st.session_state['eda_state']:
            df = st.session_state['eda_state']['current_data']
            # '.' ê¸°ì¤€ ë¬¸ì¥ ì¶”ê°€ ë¶„ë¦¬
            df['changed_sent_text'] = df['sent_text'].apply(lambda x: split_sentences(x))
            df = df.explode('changed_sent_text').reset_index(drop=True)
            st.session_state['eda_state']['current_data'] = df
            st.success("DataFrame ë¬¸ì¥ë¶„ë¦¬ ì™„ë£Œ!!")
            # df ì¶œë ¥
            st.dataframe(st.session_state['eda_state']['current_data'])

# ê³µí†µ ì „ì²˜ë¦¬ tab í•¨ìˆ˜
def preprocess1():
    if 'preprocess_started' not in st.session_state['eda_state']:
        st.session_state['eda_state']['preprocess_started'] = False
    if 'save_clicked' not in st.session_state['eda_state']:
        st.session_state['eda_state']['save_clicked'] = False
        
    if st.button('ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘'):
        st.session_state['eda_state']['preprocess_started'] = True
        
    # Initialize state if not already done
    if 'name' not in st.session_state:
        st.session_state['eda_state']['name'] = None
    
    # if 'preprocess_started' in st.session_state['eda_state'] and st.session_state['eda_state']['preprocess_started']:
    if st.session_state['eda_state']['preprocess_started']:
        with st.spinner('ì „ì²˜ë¦¬ ì‘ì—… ì¤‘...'):
            # Ensure that current_data is available in the session state
            if 'current_data' in st.session_state['eda_state']:
                try:
                    df = st.session_state['eda_state']['current_data']
                    # ì†Œë¬¸ìë¡œ ë³€í™˜
                    df['changed_sent_text']= df['changed_sent_text'].str.lower()
                    # ê³µë°± ì œê±°
                    df['changed_sent_text'] = df['changed_sent_text'].str.strip()
                    # ê´„í˜¸ ì•ˆ ë¬¸ì ì²˜ë¦¬
                    df['changed_sent_text'] = df['changed_sent_text'].apply(brackets_clean)
                    st.success("ë¬¸ì¥ ì „ì²˜ë¦¬(ê´„í˜¸ ë‚´ ë¬¸ì ì²˜ë¦¬, ì†Œë¬¸ì ë³€í™˜, ê³µë°±ì œê±°) ì™„ë£Œ!!")

                    # ë‹¨ì–´ ë³€í™˜ 1 : word_mapì—ì„œ ë‹¨ì–´ ê¸¸ì´ê°€ ê¸´ ìˆœìœ¼ë¡œ ë¨¼ì € ë³€í™˜ ì‹¤ì‹œ, re-tubingì€ ë¦¬íŠœë¹™ìœ¼ë¡œ, tubingì€ íŠœë¹™ìœ¼ë¡œ ì¸ì‹ë˜ë„ë¡ í•¨
                    df['changed_sent_text'] = df['changed_sent_text'].apply(apply_replacement1)
                    st.success("ë‹¨ì–´ ë³€í™˜1 ì™„ë£Œ!!")

                    # ë‹¨ì–´ë³€í™˜2 : ë‹¨ì–´ë³€í™˜1ì—ì„œ ì¸ì‹ë˜ì§€ ì•Šì€ ë‹¨ì–´, ì˜ˆë¥¼ë“¤ì–´ ì „ì²´tube ì™€ ê°™ì€ ë¬¸êµ¬ ì²˜ë¦¬
                    df['changed_sent_text'] = df['changed_sent_text'].apply(apply_replacement2)
                    st.success("ë‹¨ì–´ ë³€í™˜2 ì™„ë£Œ!!")

                    # ë¶€ì‹ìœ¨ ì¸ì‹ë˜ë„ë¡ ì²˜ë¦¬
                    # 'changed_sent_text' ì—´ì— í•¨ìˆ˜ë¥¼ ì ìš©í•˜ì—¬ cr_word_mapì— ë”°ë¼ ìˆ«ìë¥¼ ë‹¨ì–´ë¡œ ë°”ê¿‰ë‹ˆë‹¤.
                    df['changed_sent_text'] = df['changed_sent_text'].apply(lambda x: replace_with_words(x, cr_word_map))
                    st.success("ë¶€ì‹ìœ¨ ë³€í™˜ ì™„ë£Œ!!")

                    # í† í°í™”ë¥¼ ìœ„í•´ í•œê¸€ ë¬¸ìë§Œ ì¶”ì¶œ
                    df['changed_sent_text'] = df['changed_sent_text'].apply(extract_korean)
                    st.success("í•œê¸€ ë¬¸ì ì¶”ì¶œ ì™„ë£Œ!!")

                    # ë¹ˆí–‰ ì œê±°
                    df = df[df['changed_sent_text'].str.strip() != '']
                    st.success("ë¹„ì–´ìˆëŠ” í–‰ ì œê±° ì™„ë£Œ!!")

                    # ì„¸ì…˜ ì €ì¥
                    st.session_state['eda_state']['current_data'] = df
                    st.success("ëª¨ë“  ì „ì²˜ë¦¬ ì™„ë£Œ!!")
                    # df ì¶œë ¥
                    st.dataframe(df)

                    st.divider()
                    
                    st.session_state['eda_state']['name'] = st.text_input("ì €ì¥í•  íŒŒì¼ëª…ì„ ì…ë ¥í•˜ì„¸ìš” ex) ì „ì²˜ë¦¬ëœ 306 ta report:", st.session_state['eda_state']['name'])

                    st.markdown('''
                            â€» ì£¼ì˜ì‚¬í•­ : ë°˜ë“œì‹œ 'ë°ì´í„° ì €ì¥' ë²„íŠ¼ ëˆ„ë¥´ê¸° ì „ 'ì €ì¥í•  íŒŒì¼ëª…'ì„ ì…ë ¥ í›„ 'Enter'í‚¤ë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”.
                    ''')

                    if st.button('ë°ì´í„° ì €ì¥'):
                        st.session_state['eda_state']['save_clicked'] = True

                    # íŒŒì¼ ì €ì¥
                    # if 'save_clicked' in st.session_state['eda_state'] and st.session_state['eda_state']['save_clicked']:
                    # if st.session_state['eda_state']['save_clicked']:
                    if st.session_state['eda_state']['save_clicked'] and 'current_data' in st.session_state['eda_state']:
                        with st.spinner('ë°ì´í„° ì €ì¥ ì¤‘...'):
                            # Ensure the directory exists
                            output_dir = "./ì „ì²˜ë¦¬ëœ ta report by Streamlit"
                            if not os.path.exists(output_dir):
                                os.makedirs(output_dir)

                            # Create the full file path
                            file_path = os.path.join(output_dir, f"{st.session_state['eda_state']['name']}.csv")

                            # Save to Excel
                            df.to_csv(file_path, index=False)
                            st.success(f"íŒŒì¼ì´ {file_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!!")

                except Exception as e:
                    st.error("ì—ëŸ¬ ë°œìƒ : ", e)
    else:
        st.error("ì „ì²˜ë¦¬ í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!!")

            
# ì¶”ê°€ ì „ì²˜ë¦¬ for ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ tab ì¶œë ¥ í•¨ìˆ˜
def preprocess2():     
    # íŒŒì¼ ì—…ë¡œë”
    # ìµœëŒ€ ìš©ëŸ‰ì€ ì„œë²„ ì„¤ì •ì—ì„œ ë³€ê²½ ê°€ëŠ¥
    # https://docs.streamlit.io/library/advanced-features/configuration#set-configuration-options
    uploaded_file = st.file_uploader("ì „ì²˜ë¦¬ëœ csv íŒŒì¼ì„ ì—…ë¡œë“œ í•˜ì„¸ìš”.", type=['csv'], key='unique_preprocess2_uploader')
    if uploaded_file is not None:
        # ìƒˆ íŒŒì¼ ì—…ë¡œë“œ ì‹œ ê¸°ì¡´ ìƒíƒœ ì´ˆê¸°í™”
        st.session_state['modeling_state'] = {}
        st.session_state['using_state'] = {}
        
        # ë°ì´í„°í”„ë ˆì„ ìƒì„±
        df = pd.read_csv(uploaded_file)
        st.session_state['eda_state']['ml_data'] = df
        st.write(df)
        
    # ìƒˆë¡œ ì—…ë¡œë“œëœ íŒŒì¼ ë¡œë“œ
    if 'ml_data' in st.session_state['eda_state']:
        with st.spinner('ì „ì²˜ë¦¬ ì¤‘...'): 
            df = st.session_state['eda_state']['ml_data']
            # í† í°í™”, ë¶ˆìš©ì–´ ì œê±°
            df['changed_sent_text'] = df['changed_sent_text'].astype(str).apply(tokenize)
            df['changed_sent_text'] = df['changed_sent_text'].apply(lambda x: [item for item in x if item not in stopwords])
            # ë°ì´í„°ê°€ 2ê¸€ì ì´í•˜ì¸ í–‰ ì‚­ì œ
            df = df[df['changed_sent_text'].apply(len) > 2]
            df = df.reset_index(drop = True)
            # ë¹ˆí–‰ ì œê±°
            df = df[df['changed_sent_text'].str.strip() != '']

            # ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
            st.session_state['eda_state']['ml_data'] = df
            st.success("ë¨¸ì‹ ëŸ¬ë‹ì„ ìœ„í•œ í† í°í™”, ë¶ˆìš©ì–´ì œê±°, 2ê¸€ì ì´í•˜ì˜ í–‰ ì œê±°, ë¹„ì–´ìˆëŠ” í–‰ ì œê±° ì™„ë£Œ!!")
            
            st.divider()
            
            # df ì¶œë ¥
            st.dataframe(st.session_state['eda_state']['ml_data'])
            
            st.divider()
            
            with st.spinner('ë°ì´í„° ë¶„í•  ì¤‘...'):    
                # ë¨¸ì‹ ëŸ¬ë‹ ì ìš©ì„ ìœ„í•œ ë²¡í„°í™”
                df = st.session_state['eda_state']['ml_data']
                cnt_vect = CountVectorizer()
                bow_vect = cnt_vect.fit_transform(df['changed_sent_text'].astype(str))
                word_list = cnt_vect.get_feature_names_out()
                count_list = bow_vect.toarray().sum(axis=0)
                tag_df = df.drop(['filename','changed_sent_text', 'sent_text'],axis=1,inplace=False)
                feature_df = bow_vect
            
                # train, test ë¶„ë¦¬
                # ì›ë³¸ ë°ì´í„° í”„ë ˆì„ì˜ ì¸ë±ìŠ¤ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
                indices = df.index
                x_train, x_test, y_train, y_test, idx_train, idx_test = train_test_split(
                    feature_df, 
                    tag_df, 
                    indices, 
                    test_size=0.3, 
                    random_state=0
                )

                # x_testì— í•´ë‹¹í•˜ëŠ” ì›ë³¸ ë°ì´í„°ì…‹ì˜ í–‰ ì¸ë±ìŠ¤
                bow_vect_df = pd.DataFrame(bow_vect)
                cond = bow_vect_df.iloc[idx_test].index
                df_xtest = df[['filename','changed_sent_text']].iloc[cond]

                # ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
                st.session_state['modeling_state'] = {
                    'x_train1': x_train,
                    'y_train1': y_train,
                    'x_test1': x_test,
                    'y_test1': y_test,
                    'df_xtest': df_xtest
                }
                st.success('ë¶„í•  ì™„ë£Œ')
                
                st.divider()
                
                st.write(f"x_train: {x_train.shape}, y_train: {y_train.shape}, x_test: {x_test.shape}, y_test: {y_test.shape}")
                
                st.divider()
                
                st.dataframe(df_xtest)

# ëª¨ë¸ ìƒì„±
class DNNModel(nn.Module):
    def __init__(self):
        super().__init__()
        
        # ì‹ ê²½ë§ ë ˆì´ì–´ë¥¼ ì •ì˜
        self.fc1 = nn.Linear(1000, 128)  # ì²« ë²ˆì§¸ íˆë“  ë ˆì´ì–´
        self.fc2 = nn.Linear(128, 64)    # ë‘ ë²ˆì§¸ íˆë“  ë ˆì´ì–´
        self.fc3 = nn.Linear(64, 32)    # ë‘ ë²ˆì§¸ íˆë“  ë ˆì´ì–´
        self.fc4 = nn.Linear(32, 16)    # ë‘ ë²ˆì§¸ íˆë“  ë ˆì´ì–´
        # self.fc5 = nn.Linear(32, 16)    # ì„¸ ë²ˆì§¸ íˆë“  ë ˆì´ì–´
        # self.fc5 = nn.Linear(32, 16)    # ì„¸ ë²ˆì§¸ íˆë“  ë ˆì´ì–´
        # self.fc6 = nn.Linear(32, 16)    # ì„¸ ë²ˆì§¸ íˆë“  ë ˆì´ì–´
        self.output = nn.Linear(16, 1)  # ì¶œë ¥ ë ˆì´ì–´
        
        # í™œì„±í™” í•¨ìˆ˜ë¥¼ ì •ì˜
        self.relu = nn.ReLU()           # ReLU í™œì„±í™” í•¨ìˆ˜
        self.sigmoid = nn.Sigmoid()     # ì‹œê·¸ëª¨ì´ë“œ í™œì„±í™” í•¨ìˆ˜
        
        # ë ˆì´ì–´ì˜ ê°€ì¤‘ì¹˜ë¥¼ Xavier uniform ë°©ì‹ìœ¼ë¡œ ì´ˆê¸°í™”
        nn.init.xavier_uniform_(self.fc1.weight)
        nn.init.xavier_uniform_(self.fc2.weight)
        nn.init.xavier_uniform_(self.fc3.weight)
        nn.init.xavier_uniform_(self.fc4.weight)
        # nn.init.xavier_uniform_(self.fc5.weight)
        # nn.init.xavier_uniform_(self.fc6.weight)
        nn.init.xavier_uniform_(self.output.weight)
        
    def forward(self, x):
        # ìˆœì „íŒŒë¥¼ ì •ì˜
        out = self.relu(self.fc1(x))    # ì²« ë²ˆì§¸ ë ˆì´ì–´ë¥¼ í†µê³¼í•œ ë’¤ ReLU ì ìš©
        out = self.relu(self.fc2(out))  # ë‘ ë²ˆì§¸ ë ˆì´ì–´ë¥¼ í†µê³¼í•œ ë’¤ ReLU ì ìš©
        out = self.relu(self.fc3(out)) 
        out = self.relu(self.fc4(out))
        # out = self.relu(self.fc5(out))
        # out = self.relu(self.fc6(out))
        out = self.sigmoid(self.output(out))  # ì¶œë ¥ ë ˆì´ì–´ë¥¼ í†µê³¼í•œ ë’¤ ì‹œê·¸ëª¨ì´ë“œ ì ìš©
        return out
    
# ë”¥ëŸ¬ë‹ CustomDataset í´ë˜ìŠ¤ ìƒì„±
class CustomDataset(Dataset):
    def __init__(self, x, y):
        super().__init__() # ë¶€ëª¨ í´ë˜ìŠ¤ì˜ ìƒì„±ìë¥¼ í˜¸ì¶œ
        
        self.x = x # ë…ë¦½ ë³€ìˆ˜(ì…ë ¥ ë°ì´í„°)
        self.y = y # ì¢…ì† ë³€ìˆ˜(ë ˆì´ë¸”)
        
    def __len__(self):
        return len(self.x) # ë°ì´í„°ì…‹ì˜ ì „ì²´ ê¸¸ì´ë¥¼ ë°˜í™˜
    
    def __getitem__(self, idx):
        return self.x[idx], self.y[idx] # ì¸ë±ìŠ¤ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ì™€ ë ˆì´ë¸”ì„ ë°˜í™˜  

# ì‚¬ì „ í† í°í™”ëœ ë°ì´í„°ì— ëŒ€í•œ ìƒì„±ê¸° í•¨ìˆ˜ë¥¼ ì •ì˜    
def tokens_generator(data):
    for tokens in data:
        yield tokens
        
# ì¶”ê°€ ì „ì²˜ë¦¬ for ë”¥ëŸ¬ë‹ ëª¨ë¸ tab ì¶œë ¥ í•¨ìˆ˜
def preprocess3():     
    # íŒŒì¼ ì—…ë¡œë”
    # ìµœëŒ€ ìš©ëŸ‰ì€ ì„œë²„ ì„¤ì •ì—ì„œ ë³€ê²½ ê°€ëŠ¥
    # https://docs.streamlit.io/library/advanced-features/configuration#set-configuration-options
    uploaded_file = st.file_uploader("ì „ì²˜ë¦¬ëœ csv íŒŒì¼ì„ ì—…ë¡œë“œ í•˜ì„¸ìš”.", type=['csv'], key='unique_preprocess3_uploader')
    if uploaded_file is not None:
         # ìƒˆ íŒŒì¼ ì—…ë¡œë“œ ì‹œ ê¸°ì¡´ ìƒíƒœ ì´ˆê¸°í™”
        st.session_state['modeling_state'] = {}
        st.session_state['using_state'] = {}
        
        # ë°ì´í„°í”„ë ˆì„ ìƒì„±
        df = pd.read_csv(uploaded_file)
        st.session_state['eda_state']['dp_data'] = df
        st.write(df)

    # ìƒˆë¡œ ì—…ë¡œë“œëœ íŒŒì¼ ë¡œë“œ
    if 'dp_data' in st.session_state['eda_state']:
        with st.spinner('ì¶”ê°€ ì „ì²˜ë¦¬ ì¤‘...'):
            df = st.session_state['eda_state']['dp_data']
            # í† í°í™”, ë¶ˆìš©ì–´ ì œê±°
            df['changed_sent_text'] = df['changed_sent_text'].astype(str).apply(tokenize1)
            df['changed_sent_text'] = df['changed_sent_text'].apply(lambda x: [item for item in x if item not in stopwords])
            # ë°ì´í„°ê°€ 2ê¸€ì ì´í•˜ì¸ í–‰ ì‚­ì œ
            df = df[df['changed_sent_text'].apply(len) > 2]
            df = df.reset_index(drop = True)
            # ë¹ˆí–‰ ì œê±°
            df = df[df['changed_sent_text'].str.strip() != '']

            # ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
            st.session_state['eda_state']['dp_data'] = df
            st.success("ë”¥ëŸ¬ë‹ì„ ìœ„í•œ í† í°í™”, ë¶ˆìš©ì–´ì œê±°, 2ê¸€ì ì´í•˜ì˜ í–‰ ì œê±°, ë¹„ì–´ìˆëŠ” í–‰ ì œê±° ì™„ë£Œ!!")
            # df ì¶œë ¥
            st.dataframe(df)
    
            # train, test ë¶„ë¦¬
            # ì›ë³¸ ë°ì´í„° í”„ë ˆì„ì˜ ì¸ë±ìŠ¤ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
            with st.spinner('ë°ì´í„° ë¶„í•  ì¤‘...'):
                train_size = int(df.shape[0]*0.8) # ì „ì²´ ë°ì´í„°ì˜ 80%ë¥¼ í•™ìŠµ ë°ì´í„° í¬ê¸°ë¡œ ì„¤ì •
                train = df.sample(len(df), random_state=0)[:train_size] # í•™ìŠµ ë°ì´í„°ë¥¼ ë¬´ì‘ìœ„ë¡œ ì¶”ì¶œ
                test = df.sample(len(df), random_state=0)[train_size:] # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë¬´ì‘ìœ„ë¡œ ì¶”ì¶œ
                st.session_state['eda_state'] = {
                    'train' : train,
                    'test' : test,
                }
                # ë…ë¦½ë³€ìˆ˜, ì¢…ì†ë³€ìˆ˜ ë¶„í•  í›„ arrayë¡œ ë³€í™˜
                x_train, y_train = train['changed_sent_text'].values, train['tag'].values
                x_test, y_test = test['changed_sent_text'].values, test['tag'].values

            # ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
            st.session_state['modeling_state'] = {
                'x_train2': x_train,
                'y_train2': y_train,
                'x_test2': x_test,
                'y_test2': y_test,
            }

            st.success('ë¶„í•  ì™„ë£Œ')
        st.write(f"train, test í˜•íƒœ í™•ì¸ ... x_train: {x_train.shape}, y_train: {y_train.shape}, x_test: {x_test.shape}, y_test: {y_test.shape}")
        
        st.divider()
        
        # tag value_counts() ì‹œê°í™”
        def plot_value_counts(df, title):
            plt.figure(figsize=(10, 6))
            sns.barplot(x=df.index, y=df.values)
            plt.title(title)
            plt.ylabel('Frequency')
            plt.xlabel('Tags')
            st.pyplot(plt)
            
        with st.expander('Train and Test ë°ì´í„°ì…‹ tag ë¹ˆë„ í™•ì¸', expanded=False):
            train_tag_counts = train['tag'].value_counts(normalize=True)
            test_tag_counts = test['tag'].value_counts(normalize=True)

            # Displaying the dataframes
            pd.set_option('display.max_colwidth', None)
            st.subheader("Train ë°ì´í„°ì…‹ tag ë¹ˆë„")
            plot_value_counts(train_tag_counts, "Train Set")

            st.subheader("Test ë°ì´í„°ì…‹ tag ë¹ˆë„")
            plot_value_counts(test_tag_counts, "Test Set")
        
        if ('x_train2' in st.session_state['modeling_state'] and
            'y_train2' in st.session_state['modeling_state'] and
            'x_test2' in st.session_state['modeling_state'] and
            'y_test2' in st.session_state['modeling_state']):
            
            x_train = st.session_state['modeling_state']['x_train2']
            y_train = st.session_state['modeling_state']['y_train2']
            x_test = st.session_state['modeling_state']['x_test2']
            y_test = st.session_state['modeling_state']['y_test2']
            
            # í•™ìŠµ ë°ì´í„°ì™€ ê²€ì¦ ë°ì´í„°ë¥¼ CustomDataset ì¸ìŠ¤í„´ìŠ¤ë¡œ ë³€í™˜ í›„ ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
            st.session_state['modeling_state']['train_set'] = CustomDataset(x_train, y_train)
            st.session_state['modeling_state']['test_set'] = CustomDataset(x_test, y_test)
            st.success('í•™ìŠµë°ì´í„°, ê²€ì¦ë°ì´í„° CustomDataset ì¸ìŠ¤í„´ìŠ¤ë¡œ ë³€í™˜ ì™„ë£Œ')
        
            # ì´í„°ë ˆì´í„°ì—ì„œ ì–´íœ˜ë¥¼ êµ¬ì¶•
            vocab = build_vocab_from_iterator(
                iterator=tokens_generator(x_train), # í›ˆë ¨ ë°ì´í„°ë¡œë¶€í„° í† í°ì˜ ì´í„°ë ˆì´í„°ë¥¼ ìƒì„±
                max_tokens=1000, # ìµœëŒ€ í† í° ìˆ˜ë¥¼ ì§€ì •
                specials=['<unk>', '<sos>', '<eos>', '<pad>'] # íŠ¹ìˆ˜ í† í°ì„ ì§€ì •
            )

            # ì•Œ ìˆ˜ ì—†ëŠ” í† í°ì— ëŒ€í•œ ê¸°ë³¸ ì¸ë±ìŠ¤ë¥¼ ì„¤ì •
            vocab.set_default_index(vocab['<unk>'])
        
            # ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
            st.session_state['modeling_state']['vocab'] = vocab
            st.success('ë‹¨ì–´ì‚¬ì „ ìƒì„± ì™„ë£Œ')
            st.write(f'ë‹¨ì–´ì‚¬ì „ ê°œìˆ˜ : {len(vocab.get_stoi())}ê°œ')
            
# EDA í˜ì´ì§€ ì¶œë ¥ í•¨ìˆ˜
def eda_page():
    st.title('Exploratory Data Analysis')
    
    # eda page tab ì„¤ì •
    t1, t2, t3, t4 = st.tabs(['íŒŒì¼ ì—…ë¡œë“œ ë° í˜•ì‹ ë³€ê²½', 'ë°ì´í„° ì „ì²˜ë¦¬', 'ì¶”ê°€ ì „ì²˜ë¦¬ for ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸', 'ì¶”ê°€ ì „ì²˜ë¦¬ for ë”¥ëŸ¬ë‹ ëª¨ë¸'])
    
    with t1:
        file_uploader()
    
    with t2:
        preprocess1()
    
    with t3:
        preprocess2()
        
    with t4:
        preprocess3()
    
# train_model
def train_model1(selected_model, model_name):
    if ('x_train1' in st.session_state['modeling_state'] and 
        'x_test1' in st.session_state['modeling_state'] and 
        'y_train1' in st.session_state['modeling_state'] and 
        'y_test1' in st.session_state['modeling_state']):

        x_train = st.session_state['modeling_state']['x_train1']
        x_test = st.session_state['modeling_state']['x_test1']
        y_train = st.session_state['modeling_state']['y_train1']
        y_test = st.session_state['modeling_state']['y_test1']
        
        with st.spinner('í•™ìŠµ ì¤‘...'): 
            model = LogisticRegression(random_state = 0)
            model.fit(x_train, y_train)
        st.success('í•™ìŠµ ì™„ë£Œ')

        with st.spinner('ì˜ˆì¸¡ ê°’ ìƒì„± ì¤‘...'):
            train_pred = model.predict(x_train)
            test_pred = model.predict(x_test)
        st.success('ì˜ˆì¸¡ ê°’ ìƒì„± ì™„ë£Œ')
        
        # ëª¨ë¸ ì €ì¥ ê²½ë¡œ ì„¤ì •
        models_dir = './streamlit models'
        if not os.path.exists(models_dir):
            os.makedirs(models_dir)

        file_name = datetime.datetime.now().strftime('%Y%m%d%H%M')

        # ëª¨ë¸ íŒŒì¼ ì €ì¥
        with open(f'{models_dir}/model_{model_name.replace(" ", "_")}_{file_name}.dat', 'wb') as f:
            pickle.dump(model, f)

        return model, y_train, train_pred, y_test, test_pred
        
# ë¨¸ì‹ ëŸ¬ë‹ modeling tab ì¶œë ¥ í•¨ìˆ˜
def modeling1():
    model_list = ['Select Model', 'Logistic Regression']
    model_dict = {'Logistic Regression': LogisticRegression}
    selected_model = ''
    
    # Initialize state if not already done
    if 'selected_model' not in st.session_state:
        st.session_state['eda_state']['selected_model'] = None
        
    selected_model = st.selectbox('í•™ìŠµì— ì‚¬ìš©í•  ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”.', model_list, index=0)
    
    if selected_model in model_list[1:]:
        with st.spinner('í•™ìŠµ ì¤‘...'): 
            st.session_state['modeling_state']['selected_model'] = selected_model

            result = train_model1(model_dict[selected_model], selected_model)
            if result is not None:
                model, y_train, train_pred, y_test, test_pred = result
                st.session_state['modeling_state']['model1'] = model
                st.session_state['modeling_state']['y_train1'] = y_train
                st.session_state['modeling_state']['y_test1'] = y_test
                st.session_state['modeling_state']['train_pred1'] = train_pred
                st.session_state['modeling_state']['test_pred1'] = test_pred
                st.success('í•™ìŠµ ì¢…ë£Œ')
            else:
                st.error('Model training failed or returned unexpected results.')  
            
# ML ê²°ê³¼ tab í•¨ìˆ˜
def ml_results():
    with st.expander('Metrics', expanded=True):
        if 'y_train1' in st.session_state['modeling_state']:
            st.divider()
            st.caption('Train Results')
            c1, c2, c3 = st.columns(3)
            left, right = c1.columns(2)
            ac = accuracy_score(st.session_state['modeling_state']['y_train1'], st.session_state['modeling_state']['train_pred1'])
            left.write('**:blue[Accuracy]**')
            right.write(f'{ac: 10.5f}')

            left, right = c2.columns(2)
            recall = recall_score(st.session_state['modeling_state']['y_train1'], st.session_state['modeling_state']['train_pred1'])
            left.write('**:blue[Recall]**')
            right.write(f'{recall: 10.5f}')

            left, right = c3.columns(2)
            f1 = f1_score(st.session_state['modeling_state']['y_train1'], st.session_state['modeling_state']['train_pred1'])
            left.write('**:blue[F1]**')
            right.write(f'{f1: 10.5f}')
        if 'y_test1' in st.session_state['modeling_state']:
            st.divider()
            st.caption('Test Results')
            c1, c2, c3 = st.columns(3)
            left, right = c1.columns(2)
            ac = accuracy_score(st.session_state['modeling_state']['y_test1'], st.session_state['modeling_state']['test_pred1'])
            left.write('**:blue[Accuracy]**')
            right.write(f'{ac: 10.5f}')

            left, right = c2.columns(2)
            recall = recall_score(st.session_state['modeling_state']['y_test1'], st.session_state['modeling_state']['test_pred1'])
            left.write('**:blue[Recall]**')
            right.write(f'{recall: 10.5f}')

            left, right = c3.columns(2)
            f1 = f1_score(st.session_state['modeling_state']['y_test1'], st.session_state['modeling_state']['test_pred1'])
            left.write('**:blue[F1]**')
            right.write(f'{f1: 10.5f}')
        
    st.divider()
    
    with st.expander('Result Visualization', expanded=False):
        if 'y_train1' in st.session_state['modeling_state']:
            
            confu = confusion_matrix(y_true = st.session_state['modeling_state']['y_test1'], y_pred = st.session_state['modeling_state']['test_pred1'])

            plt.figure(figsize=(4, 3))
            plot = sns.heatmap(confu, annot=True, annot_kws={'size':15}, cmap='OrRd', fmt='.10g')
            plt.title('Confusion Matrix')
            plt.ylabel('Actual')
            plt.xlabel('Predicted')
            fig = plot.get_figure()
            st.pyplot(fig)
            # Clear the current plot to avoid overlap with future plots
            plt.clf()
            
    st.divider()
    
    # ì˜ëª» ì˜ˆì¸¡í•œ ê°’ í™•ì¸
    with st.expander('Result of incorrect prediciton', expanded=False):
        if 'df_xtest' in st.session_state['modeling_state']:
            # df['changed_sent_text'] ì¤‘ x_test í™•ì¸ ìš©ë„
            df_xtest = st.session_state['modeling_state']['df_xtest']
            df_xtest = df_xtest.reset_index(drop=True)
        if 'x_test1' in st.session_state['modeling_state']:   
            x_test = st.session_state['modeling_state']['x_test1']
            x_test = pd.DataFrame(x_test)
        if 'y_test1' in st.session_state['modeling_state']: 
            y_test = st.session_state['modeling_state']['y_test1']
            y_test = pd.DataFrame(y_test).reset_index(drop=True)
        if 'test_pred1' in st.session_state['modeling_state']:   
            y_pred = st.session_state['modeling_state']['test_pred1']
            y_pred = pd.DataFrame(y_pred)

            df_xtest['x_test'] = x_test
            df_xtest['y_test'] = y_test
            df_xtest['y_pred'] = y_pred

            falsePositive = df_xtest[['changed_sent_text','y_test','y_pred']].loc[(df_xtest['y_test'] == 0) & (df_xtest['y_pred'] == 1)]
            falseNegative = df_xtest[['changed_sent_text','y_test','y_pred']].loc[(df_xtest['y_test'] == 1) & (df_xtest['y_pred'] == 0)]

            # pd.set_option('display.max_colwidth', None)
            # df ì¶œë ¥
            st.dataframe(falsePositive)
            st.dataframe(falseNegative)

# ë¯¸ë¦¬ í•™ìŠµëœ BiLSTM ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
def load_model(filepath):
    """
    Load the model from the given filepath and return it.
    """
    checkpoint = torch.load(filepath, map_location=torch.device('cpu'))
    model = DNNModel()  # Initialize your model
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()

    return model

# ê°€ì ¸ì˜¨ ëª¨ë¸ì„ ì‹¤í–‰
def model_upload():
    # Specify the path to your model
    model_filepath = './streamlit models/dnn_model.pt'

    if os.path.exists(model_filepath):
        if st.button('ëª¨ë¸ ìƒì„±'):
            # Load the model
            model = load_model(model_filepath)
            st.session_state['modeling_state']['dp_model'] = model
            st.success("ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° ì™„ë£Œ")
            # Now you can use the model for predictions or further processing
    else:
        st.error("Model file not found. Please check the file path.")

# ëª¨ë¸ í‰ê°€ í•¨ìˆ˜
def evaluate_model(model, data_loader, loss_function, device, return_labels=False):
    model = st.session_state['modeling_state']['dp_model'].to(device)
    # # ì†ì‹¤í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì €
    loss = nn.BCELoss().to(device)
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    model.eval()
    total_loss, total_accuracy, total_f1, total_recall = 0, 0, 0, 0
    total_samples = len(data_loader.dataset)
    true_labels_list, predicted_labels_list = [], []
    
    # ê²€ì¦ íŒŒíŠ¸
    with torch.no_grad():
        for x, y in data_loader:
            x = x.to(torch.float32).to(device)
            y = y.to(torch.float32).to(device)

            outputs = model(x)
            cost = loss(outputs, y.view(-1, 1))

            total_loss += cost.item()
            predictions = (outputs > 0.5).to(torch.float32)
            correct_preds = (predictions == y.view(-1, 1)).sum().item()

            total_accuracy += correct_preds
            total_f1 += f1_score(y.cpu(), predictions.cpu())
            total_recall += recall_score(y.cpu(), predictions.cpu())

            if return_labels:
                true_labels_list.extend(y.view(-1).tolist())
                predicted_labels_list.extend(predictions.view(-1).tolist())

    avg_loss = total_loss / len(data_loader)
    avg_accuracy = total_accuracy / total_samples
    avg_f1 = total_f1 / len(data_loader)
    avg_recall = total_recall / len(data_loader)

    if return_labels:
        return avg_loss, avg_accuracy, avg_f1, avg_recall, true_labels_list, predicted_labels_list
    else:
        return avg_loss, avg_accuracy, avg_f1, avg_recall   
    
# ë²¡í„°í™”, íŒ¨ë”©        
def sent2seq(token_list, vocab):
    # ì£¼ì–´ì§„ í† í° ë¦¬ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¨ì–´ ì‚¬ì „ì— ê¸°ë°˜í•œ ì¸ë±ìŠ¤ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    # ê° í† í°ì€ ë‹¨ì–´ ì‚¬ì „ì— í•´ë‹¹í•˜ëŠ” ìˆ«ì ì¸ë±ìŠ¤ë¡œ ë³€í™˜ë©ë‹ˆë‹¤.
    seq = [vocab[token] for token in token_list]
    return seq

def vectorize_seq(sequences, dimension=1000):
    # ê° ì‹œí€€ìŠ¤ì— ëŒ€í•œ ì›-í•« ì¸ì½”ë”© ë²¡í„°ë¥¼ ë§Œë“­ë‹ˆë‹¤.
    # ê²°ê³¼ëŠ” (ì‹œí€€ìŠ¤ ê°œìˆ˜, ë‹¨ì–´ ì‚¬ì „ì˜ í¬ê¸°) í˜•íƒœì˜ 0ìœ¼ë¡œ ì±„ì›Œì§„ ë°°ì—´ì…ë‹ˆë‹¤.
    results = np.zeros((len(sequences), dimension))
    
    # ê° ì‹œí€€ìŠ¤ì— ëŒ€í•´, í•´ë‹¹í•˜ëŠ” ì¸ë±ìŠ¤ì˜ ìœ„ì¹˜ì— 1ì„ ì„¤ì •í•©ë‹ˆë‹¤.
    # ì´ëŠ” í•´ë‹¹ ë‹¨ì–´ê°€ ë¬¸ì¥ ë‚´ì— ì¡´ì¬í•¨ì„ í‘œì‹œí•©ë‹ˆë‹¤.
    for i, seq in enumerate(sequences):
        results[i, seq] = 1.
    return results


def collate_function(batch, vocab):
    label_list = []
    sentence_list = []
    
    for (token_list, label) in batch:
        # í† í° ë¦¬ìŠ¤íŠ¸ë¥¼ ë‹¨ì–´ ì‚¬ì „ì˜ ì¸ë±ìŠ¤ë¡œ ë³€í™˜í•œ ë’¤ í…ì„œë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        seq = torch.tensor(vectorize_seq([sent2seq(token_list, vocab)])[0])
        sentence_list.append(seq)
        label_list.append(label)
    
    # pad_sequenceë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  ì‹œí€€ìŠ¤ë¥¼ ë™ì¼í•œ ê¸¸ì´ë¡œ íŒ¨ë”©í•©ë‹ˆë‹¤.
    # 'batch_first=True'ëŠ” ë°°ì¹˜ í¬ê¸°ê°€ ë°˜í™˜ëœ í…ì„œì˜ ì²« ë²ˆì§¸ ì°¨ì›ì´ ë¨ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
    # '<pad>' í† í°ì— í•´ë‹¹í•˜ëŠ” ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ íŒ¨ë”©í•©ë‹ˆë‹¤.
    seq_list = pad_sequence(sentence_list, padding_value=vocab['<pad>'], batch_first=True)
    
    # ë ˆì´ë¸” ë¦¬ìŠ¤íŠ¸ë¥¼ í…ì„œë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    label_list = torch.tensor(label_list)
    
    return seq_list, label_list  

# ë”¥ëŸ¬ë‹ modeling tab ì¶œë ¥ í•¨ìˆ˜
def modeling2():
    if st.button('ìµœì  ëª¨ë¸ ìƒì„±'):
        if 'dp_model' in st.session_state['modeling_state']:
            # ìµœì ì˜ ì„±ëŠ¥ê°’ìœ¼ë¡œ ì €ì¥ëœ w,b ê°’ì„ ì´ìš©í•˜ê¸°
            model = st.session_state['modeling_state']['dp_model'].to(device)
            # ì†ì‹¤í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì €
            loss = nn.BCELoss().to(device)
            optimizer = optim.Adam(model.parameters(), lr=0.001)
            # # ìµœì  ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
            with st.spinner('í‰ê°€ ì¤‘...'): 
                if 'modeling_state' in st.session_state and all(key in st.session_state['modeling_state'] for key in ['train_set', 'test_set', 'vocab']):
                    train_set = st.session_state['modeling_state']['train_set']
                    test_set = st.session_state['modeling_state']['test_set']
                    vocab = st.session_state['modeling_state']['vocab']

                    # ë°ì´í„°ë¡œë” ìƒì„±
                    train_loader = DataLoader(dataset=train_set, batch_size=64, shuffle=True, drop_last=True, collate_fn=lambda batch: collate_function(batch, vocab))
                    test_loader = DataLoader(dataset=test_set, batch_size=64, collate_fn=lambda batch: collate_function(batch, vocab))
                    st.session_state['modeling_state']['train_loader'] = train_loader
                    st.session_state['modeling_state']['test_loader'] = test_loader
                    st.session_state['modeling_state']['vocab'] = vocab
                    st.success('DataLoader ìƒì„± ì™„ë£Œ')
                    
                    # Evaluation
                    train_loss, train_accuracy, train_f1, train_recall = evaluate_model(model, train_loader, loss, device)
                    test_loss, test_accuracy, test_f1, test_recall, true_labels, predicted_labels = evaluate_model(model, test_loader, loss, device, return_labels=True)
                    st.success('í‰ê°€ ì§€í‘œ ìƒì„± ì™„ë£Œ')
                    
                    # ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
                    st.session_state['modeling_state']['train_loss'] = train_loss
                    st.session_state['modeling_state']['test_loss'] = test_loss
                    st.session_state['modeling_state']['train_accuracy'] = train_accuracy
                    st.session_state['modeling_state']['test_accuracy'] = test_accuracy
                    st.session_state['modeling_state']['train_f1'] = train_f1
                    st.session_state['modeling_state']['test_f1'] = test_f1
                    st.session_state['modeling_state']['train_recall'] = train_recall
                    st.session_state['modeling_state']['test_recall'] = test_recall
                    st.session_state['modeling_state']['true_labels'] = true_labels
                    st.session_state['modeling_state']['predicted_labels'] = predicted_labels
                    
                else:
                    st.error("Modeling state not found or incomplete in the session state.")
        else:
            st.error("Model not found in the session state.")
            
# DL ê²°ê³¼ tab í•¨ìˆ˜
def dp_results():
    with st.expander('Metrics', expanded=True):
        if ('train_loss' in st.session_state['modeling_state'] and
            'test_loss' in st.session_state['modeling_state'] and
            'train_accuracy' in st.session_state['modeling_state'] and
            'test_accuracy' in st.session_state['modeling_state'] and
            'train_f1' in st.session_state['modeling_state'] and
            'test_f1' in st.session_state['modeling_state'] and
            'train_recall' in st.session_state['modeling_state'] and
            'test_recall' in st.session_state['modeling_state']):
            
            train_loss = st.session_state['modeling_state']['train_loss']
            test_loss = st.session_state['modeling_state']['test_loss']
            train_accuracy = st.session_state['modeling_state']['train_accuracy']
            test_accuracy = st.session_state['modeling_state']['test_accuracy']
            train_f1 = st.session_state['modeling_state']['train_f1']
            test_f1 = st.session_state['modeling_state']['test_f1']
            train_recall = st.session_state['modeling_state']['train_recall']
            test_recall = st.session_state['modeling_state']['test_recall']
            
            # Display results
            st.divider()
            st.caption('Train Results')
            c1, c2, c3, c4 = st.columns(4)
            left, right = c1.columns(2)
            left.write('**:blue[Loss]**')
            right.write(f'{train_loss: 10.5f}')
            
            left, right = c2.columns(2)
            left.write('**:blue[Accuracy]**')
            right.write(f'{train_accuracy: 10.5f}')

            left, right = c3.columns(2)
            left.write('**:blue[Recall]**')
            right.write(f'{train_recall: 10.5f}')

            left, right = c4.columns(2)
            left.write('**:blue[F1]**')
            right.write(f'{train_f1: 10.5f}')
        
            st.divider()
            st.caption('Test Results')
            c1, c2, c3, c4 = st.columns(4)
            left, right = c1.columns(2)
            left.write('**:blue[Val Loss]**')
            right.write(f'{test_loss: 10.5f}')
            
            left, right = c2.columns(2)
            left.write('**:blue[Val Accuracy]**')
            right.write(f'{test_accuracy: 10.5f}')

            left, right = c3.columns(2)
            left.write('**:blue[Val Recall]**')
            right.write(f'{test_recall: 10.5f}')

            left, right = c4.columns(2)
            left.write('**:blue[Val F1]**')
            right.write(f'{test_f1: 10.5f}')
        
    st.divider()
    
    with st.expander('Result Visualization', expanded=False):
        if ('true_labels' in st.session_state['modeling_state'] and
            'predicted_labels' in st.session_state['modeling_state']):
            true_labels = st.session_state['modeling_state']['true_labels']
            predicted_labels = st.session_state['modeling_state']['predicted_labels']
            
             # Visualization
            conf_matrix = confusion_matrix(true_labels, predicted_labels)
            plt.figure(figsize=(4, 3))
            sns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues')
            plt.title('Confusion Matrix')
            plt.xlabel('Predicted labels')
            plt.ylabel('True labels')
            st.pyplot(plt)
            # Clear the current plot to avoid overlap with future plots
            plt.clf()
            
    st.divider()
    
    with st.expander('Incorrect Predictions', expanded=False):
        if ('true_labels' in st.session_state['modeling_state'] and
            'predicted_labels' in st.session_state['modeling_state'] and
            'test' in st.session_state['eda_state']):
            
            true_labels = st.session_state['modeling_state']['true_labels']
            predicted_labels = st.session_state['modeling_state']['predicted_labels']
            test = st.session_state['eda_state']['test']
            test['pred_tag'] = predicted_labels

            # Initializing lists to store indices
            false_positives_indices = []
            false_negatives_indices = []

            # Iterating to categorize incorrect predictions
            for i, (true, pred) in enumerate(zip(true_labels, predicted_labels)):
                if true != pred:
                    if true == 0 and pred == 1:
                        false_positives_indices.append(i)
                    elif true == 1 and pred == 0:
                        false_negatives_indices.append(i)

            # Retrieving rows from the dataframe
            false_positives_df = test[['changed_sent_text','tag','pred_tag']].iloc[false_positives_indices]
            false_negatives_df = test[['changed_sent_text','tag','pred_tag']].iloc[false_negatives_indices]

            # Displaying the dataframes
            pd.set_option('display.max_colwidth', None)
            st.subheader("False Positives:")
            st.dataframe(false_positives_df)

            st.subheader("False Negatives:")
            st.dataframe(false_negatives_df) # Find indices where predictions were incorrect
            
    st.divider()
    
# ì…ë ¥ëœ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜
def sentiment_predict(model, device, vocab):
    key = 'text_input_key'
    if key not in st.session_state:
        st.session_state[key] = ''
        
    # Initialize 'prediction_result' if it's not already in the session state
    if 'prediction_result' not in st.session_state:
        st.session_state['prediction_result'] = None
        
    new_sentence = st.text_input('textë¥¼ ì…ë ¥í•˜ì„¸ìš”: ', key=key)
    if new_sentence != st.session_state[key]:
        st.session_state[key] = new_sentence
        st.session_state.prediction_result = None  # Reset prediction result
    if new_sentence:
        new_sentence = re.sub(r'\(([\d\W_]*?)\)', '()', new_sentence)
        new_sentence = re.sub(r'([^\s])(\()', r'\1 \2', new_sentence)
        new_sentence = re.sub(r'(\))([^\s])', r'\1 \2', new_sentence)
        new_sentence = new_sentence.lower()
        new_sentence = new_sentence.strip()
        new_sentence = apply_replacement1(new_sentence)
        new_sentence = apply_replacement2(new_sentence)
        new_sentence = replace_with_words(new_sentence, cr_word_map)
        new_sentence = extract_korean(new_sentence)
        new_sentence = tokenize1(new_sentence)
        new_sentence = [item for item in new_sentence if item not in stopwords]
        # print(new_sentence)
    
        # Convert the tokenized sentence into a format suitable for the model
        encoded = collate_function([(new_sentence, 0)], vocab)
        input_ids = encoded[0].to(device).to(torch.float32)
    
        # Get the model output
        model.eval()
        with torch.no_grad():
            h = model(input_ids)
            # print(h)

        # Output the result
        if h.item() > 0.5:
            st.session_state.prediction_result = "ì´ ë¬¸ì¥ì€ {:.2f}% í™•ë¥ ë¡œ ê¸ì •ì…ë‹ˆë‹¤".format(h.item() * 100)
        else:
            st.session_state.prediction_result = "ì´ ë¬¸ì¥ì€ {:.2f}% í™•ë¥ ë¡œ ë¶€ì •ì…ë‹ˆë‹¤".format((1 - h.item()) * 100)
                
    # Display the result if it's available
    if st.session_state.prediction_result:
        st.write(st.session_state.prediction_result)    

# Modeling í˜ì´ì§€ ì¶œë ¥ í•¨ìˆ˜
def modeling_page():
    st.title('ML & DL Modeling')
    
    # tabsë¥¼ ì¶”ê°€í•˜ì„¸ìš”.
    t1, t2, t3, t4 = st.tabs(['ML Modeling', 'ML Results', 'DL Modeling', 'DL Results'])

    # file upload tab êµ¬í˜„
    with t1:
        modeling1()
    
    with t2:
        ml_results()
        
    with t3:
        model_upload()
        modeling2()
        # Example usage
        st.divider()
        st.write('ìµœì  ëª¨ë¸ test í•˜ê¸°')
        if ('dp_model' in st.session_state['modeling_state'] and
            'vocab' in st.session_state['modeling_state']):
            model = st.session_state['modeling_state']['dp_model']
            vocab = st.session_state['modeling_state']['vocab']
            sentiment_predict(model, device, vocab)
    
    with t4:
        dp_results()
######################################################################################################
# ìƒˆë¡œìš´ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜1
def new_data_preprocess_predict(text, model, device, vocab):     
    # ìƒˆë¡œ ì—…ë¡œë“œëœ íŒŒì¼ ë¡œë“œ
    with st.spinner('ì¶”ê°€ ì „ì²˜ë¦¬ ë° ì˜ˆì¸¡ ì¤‘...'):
        # í† í°í™”, ë¶ˆìš©ì–´ ì œê±°
        text = tokenize1(text)
        text = [item for item in text if item not in stopwords]
        # ë°ì´í„°ê°€ 2ê¸€ì ì´í•˜ì¸ í–‰ ì‚­ì œ
        if len(text) <= 2:
            return None      
        
        # ëª¨ë¸ì˜ ì…ë ¥ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        encoded = collate_function([(text, 0)], vocab)
        # ë³€í™˜ëœ ë°ì´í„°ë¥¼ ë””ë°”ì´ìŠ¤(CPU ë˜ëŠ” GPU)ì— í• ë‹¹í•©ë‹ˆë‹¤.
        input_ids = encoded[0].to(device).to(torch.float32)

        # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
        model.eval()

        with torch.no_grad():
            # ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
            h = model(input_ids)
             
    # ì˜ˆì¸¡ ê²°ê³¼ì— ë”°ë¼ ê¸ì •(1) ë˜ëŠ” ë¶€ì •(0)ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    return 1 if h.item() > 0.5 else 0

# ë°ì´í„° ì‹œê°í™” í•¨ìˆ˜
# tag value_counts() ì‹œê°í™”
def plot_value_counts(df, title):
    plt.figure(figsize=(10, 6))
    sns.barplot(x=df.index, y=df.values)
    plt.title(title)
    plt.ylabel('Frequency')
    plt.xlabel('Tags')
    st.pyplot(plt)
    
# sheet_name cleaning funciton
def clean_sheet_name(name):    
    for char in['[', ']',':', '*','?','/', '\\','(', ')']:
        name =name.replace(char, '')
    # Convert name to lowercase
    name = name.lower()

    # Allow spaces only between numbers and letters
    cleaned_name = ""
    for i in range(len(name)):
        if name[i].isalnum() or (name[i] == " " and i > 0 and name[i-1].isalnum() and i < len(name)-1 and name[i+1].isalnum()):
            cleaned_name += name[i]

    return cleaned_name

# using í˜ì´ì§€ ì¶œë ¥ í•¨ìˆ˜
def using_page():
    st.title('Worklist ì‘ì„± ë° ML & DL Model ì‚¬ìš©')
    
    # tabsë¥¼ ì¶”ê°€í•˜ì„¸ìš”.
    t1, t2, t3 = st.tabs(['Worklist ì‘ì„±', 'ML Model ì‚¬ìš©', 'DL Model ì‚¬ìš©'])
    
    # file upload tab êµ¬í˜„
    with t1:
        nested_tab1, nested_tab2, nested_tab2 = st.tabs(["Worklist ê°€ê³µ1", "Worklist ê°€ê³µ2", "Worklist ì‘ì„±"])
        with nested_tab1:
            uploaded_file = st.file_uploader("Worklist ì‘ì„±ì— ì°¸ê³ í•˜ì‹¤ ê³¼ê±° Worklistë¥¼ ì—…ë¡œë“œ í•˜ì„¸ìš”.", type=["csv", "xlsx"], key = 'worklist_upload')
            if uploaded_file is not None:
                with st.spinner('ë°ì´í„° ê°€ê³µì¤‘...'): 
                    df = pd.read_excel(uploaded_file)
                    st.session_state['eda_state']['worklist_df'] = df

                    # 'ëª©ì°¨' ì—´ì—ì„œ NaN ê°’ì´ ìˆëŠ” í–‰ ì œê±°
                    df = df.dropna(subset=['ëª©ì°¨'])
                    # 'ëª©ì°¨', 'ì‘ì—…ë‚´ìš©', 'ìš”ì²­íŒ€' ì—´ë§Œ ì„ íƒ
                    df = df[['ëª©ì°¨', 'ì‘  ì—…  ë‚´  ìš©', 'ìš”ì²­íŒ€']]
                    # 'ìš”ì²­íŒ€' ì—´ í•„í„°ë§ ì¡°ê±´ ì„¤ì • ('ì¥ì¹˜Reliability1íŒ€'ì´ê±°ë‚˜ NaN)
                    cond = (df['ìš”ì²­íŒ€'] == 'ì¥ì¹˜Reliability1íŒ€') | (df['ìš”ì²­íŒ€'].isna())
                    # ì¡°ê±´ì— ë”°ë¼ 'ìš”ì²­íŒ€' ì—´ í•„í„°ë§
                    df['ìš”ì²­íŒ€'] = df['ìš”ì²­íŒ€'][cond]
                    # 'ëª©ì°¨' ì—´ì„ ë¬¸ìì—´ íƒ€ì…ìœ¼ë¡œ ë³€í™˜
                    df['ëª©ì°¨'] = df['ëª©ì°¨'].astype(str)
                    
                    # Initialize an in-memory bytes buffer
                    output_buffer = io.BytesIO()
                    
                    # ì—‘ì…€ íŒŒì¼ ì“°ê¸°ë¥¼ ìœ„í•œ pd.ExcelWriter ì„¤ì •
                    with pd.ExcelWriter(output_buffer, engine = 'xlsxwriter') as writer:
                        start_index = None  # ê·¸ë£¹ì˜ ì‹œì‘ ì¸ë±ìŠ¤ ì´ˆê¸°í™”
                        # DataFrameì˜ ê° í–‰ì— ëŒ€í•´ ë°˜ë³µ
                        for index, row in df.iterrows():
                            # íŠ¹ì • ì¡°ê±´ì„ ì¶©ì¡±í•˜ëŠ”ì§€ í™•ì¸ (ì—¬ê¸°ì„œëŠ” ë¬¸ìì—´ì´ '.0'ìœ¼ë¡œ ëë‚˜ëŠ” ê²½ìš°)
                            if str(row['ëª©ì°¨']).endswith('.0'):
                                if start_index is not None:
                                    # ì´ì „ ê·¸ë£¹ì„ ì—‘ì…€ ì‹œíŠ¸ë¡œ ì‘ì„±
                                    group_df = df.loc[start_index:index - 1]
                                    # ì²« ë²ˆì§¸ í–‰ì˜ ë°ì´í„°ë¡œë¶€í„° ì‹œíŠ¸ ì´ë¦„ ìƒì„± (ì´ë¦„ ì²­ì†Œ í•¨ìˆ˜ í•„ìš”)
                                    sheet_name = clean_sheet_name(group_df.iloc[0]['ì‘  ì—…  ë‚´  ìš©'])[:31]
                                    # ì—‘ì…€ íŒŒì¼ì— ì‘ì„±
                                    group_df.to_excel(writer, sheet_name=sheet_name, index=False)

                                # ìƒˆ ê·¸ë£¹ì˜ ì‹œì‘ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸
                                start_index = index

                        # ë§ˆì§€ë§‰ ê·¸ë£¹ì´ ì¡´ì¬í•˜ëŠ” ê²½ìš° íŒŒì¼ì— ì‘ì„±
                        if start_index is not None and start_index < len(df):
                            group_df = df.loc[start_index:]
                            # ì‹œíŠ¸ ì´ë¦„ ìƒì„± (ì´ë¦„ ì²­ì†Œ í•¨ìˆ˜ í•„ìš”)
                            sheet_name = clean_sheet_name(group_df.iloc[0]['ì‘  ì—…  ë‚´  ìš©'])[:31]
                            # ì—‘ì…€ íŒŒì¼ì— ë§ˆì§€ë§‰ ê·¸ë£¹ ì‘ì„±
                            group_df.to_excel(writer, sheet_name=sheet_name, index=False)
                            
                            st.success("Worklist ê°€ê³µ1 ì™„ë£Œ.")
                        
                    # Seek to the beginning of the stream
                    output_buffer.seek(0) 
                    # Download button for the Excel file
                    st.download_button(label="Download Excel file",
                                       data=output_buffer,
                                       file_name="ê°€ê³µ1ì™„ë£Œëœ Worklist.xlsx",
                                       mime="application/vnd.ms-excel")
                    
                    
                    
#                     def merge_excel_files(file_paths):
#                         # This dictionary will hold dataframes with sheet names as keys
#                         combined_data = {}

#                         for file in file_paths:
#                             # Load each Excel file
#                             xls = pd.ExcelFile(file)

#                             # Iterate through each sheet in the Excel file
#                             for sheet_name in xls.sheet_names:
#                                 # Read each sheet
#                                 df = pd.read_excel(xls, sheet_name)

#                                 # If the sheet name is already in the dictionary, append the new data
#                                 if sheet_name in combined_data:
#                                     combined_data[sheet_name] = combined_data[sheet_name].append(df, ignore_index=True)
#                                 else:
#                                     # If this is the first time we're seeing this sheet name, add it to the dictionary
#                                     combined_data[sheet_name] = df

#                         # Save the combined data to a new Excel file
#                         writer = pd.ExcelWriter('í†µí•©ëœ worklist.xlsx', engine='openpyxl')

#                         for sheet_name, data in combined_data.items():
#                             # Write each dataframe to a different sheet
#                             data.to_excel(writer, sheet_name=sheet_name, index=False)

#                         writer.save()

#                     # Example usage
#                     file_paths = ['./8_1. worklist ë¶„ë¦¬/23ë…„ ë¶„ë¦¬ëœ íŒŒì¼.xlsx', './8_1. worklist ë¶„ë¦¬/19ë…„ ë¶„ë¦¬ëœ íŒŒì¼.xlsx', './8_1. worklist ë¶„ë¦¬/15ë…„ ë¶„ë¦¬ëœ íŒŒì¼.xlsx']
#                     merge_excel_files(file_paths)
    
    # file upload tab êµ¬í˜„
    with t2:
        uploaded_file = st.file_uploader("ì „ì²˜ë¦¬ê°€ ì™„ë£Œëœ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ì—…ë¡œë“œ í•˜ì„¸ìš”.", type=["csv", "xlsx"], key = 'ml_upload')
        if uploaded_file is not None:
            
            df_new = pd.read_csv(uploaded_file)
            st.session_state['eda_state']['ml_verification_data'] = df_new
            st.success("ë°ì´í„°í”„ë ˆì„ ìƒì„±.")
            st.write(df_new)
            
            st.divider()
            
            with st.expander('í•™ìŠµë˜ì§€ ì•Šì€ ìƒˆë¡œìš´ ë°ì´í„°ì…‹ tag ë¹ˆë„ í™•ì¸', expanded=False):
                tag_counts = df_new['tag'].value_counts(normalize=True)

                # Displaying the dataframes
                pd.set_option('display.max_colwidth', None)
                st.subheader("í•™ìŠµë˜ì§€ ì•Šì€ ìƒˆë¡œìš´ ë°ì´í„°ì…‹ tag ë¹ˆë„")
                plot_value_counts(tag_counts, "New DataSet")
                
            st.divider()
            
            with st.spinner('í•™ìŠµ ë°ì´í„° ìƒì„± ì¤‘...'):   
                if 'ml_data' in st.session_state['eda_state']:
                    df = st.session_state['eda_state']['ml_data']

                    # ë¨¸ì‹ ëŸ¬ë‹ ì ìš©ì„ ìœ„í•œ ë²¡í„°í™”
                    df = st.session_state['eda_state']['ml_data']
                    cnt_vect = CountVectorizer()
                    bow_vect1 = cnt_vect.fit_transform(df['changed_sent_text'].astype(str))
                    word_list = cnt_vect.get_feature_names_out()
                    count_list = bow_vect1.toarray().sum(axis=0)
                    x_train = bow_vect1
                    y_train = df.drop(['filename','changed_sent_text', 'sent_text'],axis=1,inplace=False)
                    
                    st.success("í•™ìŠµ ë°ì´í„° ìƒì„± ì™„ë£Œ.")
                    
                    st.divider()
                    
                    with st.spinner('ê²€ì¦ ë°ì´í„° ìƒì„± ì¤‘...'):
                        if 'ml_verification_data' in st.session_state['eda_state']:
                            df_new = st.session_state['eda_state']['ml_verification_data']

                            df_new['changed_sent_text'] = df_new['changed_sent_text'].astype(str).apply(tokenize)
                            df_new['changed_sent_text'] = df_new['changed_sent_text'].apply(lambda x: [item for item in x if item not in stopwords])
                            df_new = df_new[df_new['changed_sent_text'].apply(len) > 2]
                            df_new = df_new.reset_index(drop = True)
                            empty_rows = df_new[df_new['changed_sent_text'].astype(str).apply(lambda x: x.strip() == '')]
                            empty_rows_index = empty_rows.index
                            df_new = df_new.drop(empty_rows_index)

                            # ê²€ì¦ ë°ì´í„° ìƒì„±
                            bow_vect2 = cnt_vect.transform(df_new['changed_sent_text'].astype(str))

                            x_valid = bow_vect2
                            y_valid = df_new.drop(['filename','changed_sent_text', 'sent_text'],axis=1,inplace=False)

                            x_valid.shape, y_valid.shape
                            
                            st.success("í† í°í™”, ë¶ˆìš©ì–´ì œê±°, 2ê¸€ì ì´í•˜ì˜ í–‰ ì œê±° ì™„ë£Œ!!")
                            st.success("ê²€ì¦ ë°ì´í„° ìƒì„± ì™„ë£Œ.")
                            
                            st.divider()
                            
                            with st.spinner('ëª¨ë¸ ìƒì„± ì¤‘...'):
                                
                                # í•™ìŠµí•˜ê¸°
                                # fit in training set
                                lr = LogisticRegression(random_state = 0)
                                lr.fit(x_train, y_train)
                                # predict in test set
                                y_pred = lr.predict(x_valid)

                                st.success('ê¸ì •, ë¶€ì • ì˜ˆì¸¡ ì™„ë£Œ.')

                                st.divider()
                        
                                with st.expander('Result Evaluation', expanded=False):

                                    st.caption('Train Results')
                                    c1, c2, c3 = st.columns(3)
                                    left, right = c1.columns(2)
                                    left.write('**:blue[Accuracy]**')
                                    right.write(f'{accuracy_score(y_valid, y_pred): 10.5f}')

                                    left, right = c2.columns(2)
                                    left.write('**:blue[Recall]**')
                                    right.write(f'{recall_score(y_valid, y_pred): 10.5f}')

                                    left, right = c3.columns(2)
                                    left.write('**:blue[F1]**')
                                    right.write(f'{f1_score(y_valid, y_pred): 10.5f}')
                                    
                                    st.divider()
                        
                                with st.expander('Result Visualization', expanded=False):

                                    # Creating the confusion matrix
                                    cm = confusion_matrix(y_valid, y_pred)

                                    # Plotting the confusion matrix
                                    plt.figure(figsize=(8, 6))
                                    plot = sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])
                                    plt.xlabel('Predicted')
                                    plt.ylabel('Actual')
                                    plt.title('Confusion Matrix of Sentiment Prediction')
                                    fig = plot.get_figure()
                                    st.pyplot(fig)
                                    # Clear the current plot to avoid overlap with future plots
                                    plt.clf()

                                    st.divider()

                                # ì˜ëª» ì˜ˆì¸¡í•œ ê°’ í™•ì¸
                                with st.expander('Result of incorrect prediciton', expanded=False):
                                    
                                    df_new['pred_tag'] = y_pred

                                    falsePositive = df_new[['changed_sent_text','tag','pred_tag']].loc[(df_new['tag'] == 0) & (df_new['pred_tag'] == 1)]
                                    falseNegative = df_new[['changed_sent_text','tag','pred_tag']].loc[(df_new['tag'] == 1) & (df_new['pred_tag'] == 0)]

                                    # pd.set_option('display.max_colwidth', None)
                                    # df ì¶œë ¥
                                    st.subheader('falsePositive')
                                    st.dataframe(falsePositive)
                                    st.subheader('falseNegative')
                                    st.dataframe(falseNegative)
                                    
                                    st.divider()
                                    
                                # filenameê¸°ì¤€ êµ¬ë£¹í™” ë° pred_tag ê¸°ì¤€ ë¶„ë¥˜í•˜ì—¬ ì‹œê°í™”
                                # Step 1: Data Preparation
                                # Group by 'filename' and then sort within groups by 'pred_tag'
                                grouped_df = df_new.groupby('filename', group_keys=False).apply(lambda x: x.sort_values('pred_tag', ascending=False))
                                
                                # ë¶€ì •ì–´ëŠ” ë¹¨ê°„ìƒ‰ìœ¼ë¡œ í‘œì‹œ
                                def style_row(row):
                                    if row['pred_tag'] == 0:
                                        return f"<tr style='font-weight: bold; color: red;'><td>{row['filename']}</td><td>{row['sent_text']}</td><td>{row['pred_tag']}</td></tr>"
                                    else:
                                        return f"<tr><td>{row['filename']}</td><td>{row['sent_text']}</td><td>{row['pred_tag']}</td></tr>"

                                # Step 2: Streamlit App
                                with st.expander('Assistance for Worklist Writing', expanded=False):

                                    # Using Session State to store the index of the current file group
                                    if 'index' not in st.session_state:
                                        st.session_state['index'] = 0

                                    # Getting unique filenames to iterate through
                                    unique_filenames = sorted(df_new['filename'].unique())

                                    # Display current group
                                    current_file = unique_filenames[st.session_state['index']]
                                    st.subheader(f"Data for {current_file}")
                                    # Convert the DataFrame to HTML for custom styling
                                    styled_html = "<table><tr><th>Filename</th><th>Sent Text</th><th>Pred Tag</th></tr>"
                                    for _, row in grouped_df[grouped_df['filename'] == current_file].iterrows():
                                        styled_html += style_row(row)
                                    styled_html += "</table>"
                                    st.markdown(styled_html, unsafe_allow_html=True)

                                    # Navigation
                                    col1, col2 = st.columns(2)
                                    with col1:
                                        if st.button("Previous"):
                                            if st.session_state['index'] > 0:
                                                st.session_state['index'] -= 1

                                    with col2:
                                        if st.button("Next"):
                                            if st.session_state['index'] < len(unique_filenames) - 1:
                                                st.session_state['index']+= 1
                            
                        else:
                            st.error("ê²€ì¦ ë°ì´í„° ìƒì„± ì‹¤íŒ¨.")
                else:
                    st.error("í•™ìŠµ ë°ì´í„° ìƒì„± ì‹¤íŒ¨.")
        else:
            st.error("ì—…ë¡œë“œëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    # file upload tab êµ¬í˜„
    with t3:
        uploaded_file = st.file_uploader("ì „ì²˜ë¦¬ê°€ ì™„ë£Œëœ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ì—…ë¡œë“œ í•˜ì„¸ìš”.", type=["csv", "xlsx"], key = 'dp_upload')
        if uploaded_file is not None:
            
            df_new = pd.read_csv(uploaded_file)
            st.session_state['eda_state']['dp_verification_data'] = df_new
            st.success("ë°ì´í„°í”„ë ˆì„ ìƒì„±.")
            st.write(df_new)
            
            st.divider()
            
            with st.expander('í•™ìŠµë˜ì§€ ì•Šì€ ìƒˆë¡œìš´ ë°ì´í„°ì…‹ tag ë¹ˆë„ í™•ì¸', expanded=False):
                tag_counts = df_new['tag'].value_counts(normalize=True)

                # Displaying the dataframes
                pd.set_option('display.max_colwidth', None)
                st.subheader("í•™ìŠµë˜ì§€ ì•Šì€ ìƒˆë¡œìš´ ë°ì´í„°ì…‹ tag ë¹ˆë„")
                plot_value_counts(tag_counts, "New DataSet")
                
            st.divider()
            
            with st.spinner('ëª¨ë¸ ìƒì„± ì¤‘...'):   
                if 'dp_model' in st.session_state['modeling_state']:
                    model = st.session_state['modeling_state']['dp_model'].to(device)
                    # ì†ì‹¤í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì €
                    loss = nn.BCELoss().to(device)
                    optimizer = optim.Adam(model.parameters(), lr=0.001)
                    st.success("Model ë¶ˆëŸ¬ì˜¤ê¸° ì„±ê³µ.")
                    
                    st.divider()
            
                    if 'vocab' in st.session_state['modeling_state']:
                        vocab = st.session_state['modeling_state']['vocab']
                        df_new['pred_tag'] = df_new['changed_sent_text'].apply(lambda x: new_data_preprocess_predict(x, model, device, vocab) if pd.notnull(x) else None)
                        st.success("í† í°í™”, ë¶ˆìš©ì–´ì œê±°, 2ê¸€ì ì´í•˜ì˜ í–‰ ì œê±° ì™„ë£Œ!!")
                        st.success("ë°ì´í„° í˜•íƒœ ë³€í™˜ ì™„ë£Œ.")
                        st.success('ê¸ì •, ë¶€ì • ì˜ˆì¸¡ ì™„ë£Œ.')
                        
                        st.divider()
                        
                        with st.expander('Result Evaluation', expanded=False):
                            
                            true_labels = df_new['tag']
                            predicted_labels = df_new['pred_tag']

                            # Calculate metrics
                            accuracy = accuracy_score(true_labels, predicted_labels)
                            recall = recall_score(true_labels, predicted_labels, pos_label=1)
                            f1 = f1_score(true_labels, predicted_labels, pos_label=1)
                            
                            st.caption('Train Results')
                            c1, c2, c3 = st.columns(3)
                            left, right = c1.columns(2)
                            left.write('**:blue[Accuracy]**')
                            right.write(f'{accuracy: 10.5f}')

                            left, right = c2.columns(2)
                            left.write('**:blue[Recall]**')
                            right.write(f'{recall: 10.5f}')

                            left, right = c3.columns(2)
                            left.write('**:blue[F1]**')
                            right.write(f'{f1: 10.5f}')
                        
                        st.divider()
                        
                        with st.expander('Result Visualization', expanded=False):
                            
                            # Ensure there are no null values in the columns used for the confusion matrix
                            df_new = df_new.dropna(subset=['tag', 'pred_tag'])

                            # Creating the confusion matrix
                            cm = confusion_matrix(df_new['tag'], df_new['pred_tag'])

                            # Plotting the confusion matrix
                            plt.figure(figsize=(8, 6))
                            plot = sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])
                            plt.xlabel('Predicted')
                            plt.ylabel('Actual')
                            plt.title('Confusion Matrix of Sentiment Prediction')
                            fig = plot.get_figure()
                            st.pyplot(fig)
                            # Clear the current plot to avoid overlap with future plots
                            plt.clf()

                        st.divider()

                        # ì˜ëª» ì˜ˆì¸¡í•œ ê°’ í™•ì¸
                        with st.expander('Result of incorrect prediciton', expanded=False):
                            
                            falsePositive = df_new[['changed_sent_text','tag','pred_tag']].loc[(df_new['tag'] == 0) & (df_new['pred_tag'] == 1)]
                            falseNegative = df_new[['changed_sent_text','tag','pred_tag']].loc[(df_new['tag'] == 1) & (df_new['pred_tag'] == 0)]

                            # pd.set_option('display.max_colwidth', None)
                            # df ì¶œë ¥
                            st.subheader('falsePositive')
                            st.dataframe(falsePositive)
                            st.subheader('falseNegative')
                            st.dataframe(falseNegative)
                            
                        st.divider()
                            
                        # filenameê¸°ì¤€ êµ¬ë£¹í™” ë° pred_tag ê¸°ì¤€ ë¶„ë¥˜í•˜ì—¬ ì‹œê°í™”
                        # Step 1: Data Preparation
                        # Group by 'filename' and then sort within groups by 'pred_tag'
                        grouped_df = df_new.groupby('filename', group_keys=False).apply(lambda x: x.sort_values('pred_tag', ascending=False))

                        # ë¶€ì •ì–´ëŠ” ë¹¨ê°„ìƒ‰ìœ¼ë¡œ í‘œì‹œ
                        def style_row(row):
                            if row['pred_tag'] == 0:
                                return f"<tr style='font-weight: bold; color: red;'><td>{row['filename']}</td><td>{row['sent_text']}</td><td>{row['pred_tag']}</td></tr>"
                            else:
                                return f"<tr><td>{row['filename']}</td><td>{row['sent_text']}</td><td>{row['pred_tag']}</td></tr>"

                        # Step 2: Streamlit App
                        with st.expander('Assistance for Worklist Writing', expanded=False):

                            # Using Session State to store the index of the current file group
                            if 'index' not in st.session_state:
                                st.session_state['index'] = 0

                            # Getting unique filenames to iterate through
                            unique_filenames = sorted(df_new['filename'].unique())

                            # Display current group
                            current_file = unique_filenames[st.session_state['index']]
                            st.subheader(f"Data for {current_file}")
                            # Convert the DataFrame to HTML for custom styling
                            styled_html = "<table><tr><th>Filename</th><th>Sent Text</th><th>Pred Tag</th></tr>"
                            for _, row in grouped_df[grouped_df['filename'] == current_file].iterrows():
                                styled_html += style_row(row)
                            styled_html += "</table>"
                            st.markdown(styled_html, unsafe_allow_html=True)

                            # Navigation
                            col1, col2 = st.columns(2)
                            with col1:
                                if st.button("Previous"):
                                    if st.session_state['index'] > 0:
                                        st.session_state['index'] -= 1

                            with col2:
                                if st.button("Next"):
                                    if st.session_state['index'] < len(unique_filenames) - 1:
                                        st.session_state['index']+= 1

                    else:
                        st.error("ë‹¨ì–´ì‚¬ì „ ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨.")

                else:
                    st.error("Model ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨.")
######################################################################################################
# Checklist í˜ì´ì§€ ì¶œë ¥ í•¨ìˆ˜
def checklist_page():
    st.title('Checklist Generator')
    
    if 'checklist_state' not in st.session_state:
        st.session_state['checklist_state'] = {}
        
    # Initialize state if not already done
    if 'grouped_file_path' not in st.session_state:
        st.session_state['checklist_state']['grouped_file_path'] = None
        
    # Initialize filename1 if not already done
    if 'filename1' not in st.session_state:
        st.session_state['checklist_state']['filename1'] = None
    
    # tabsë¥¼ ì¶”ê°€í•˜ì„¸ìš”.
    t1, t2, t3, t4 = st.tabs(['Worklist ì‹œíŠ¸ ë³„ ë¶„ë¦¬', 'Worklist í˜•íƒœ ë³€ê²½', 'Worklist ê·¸ë£¨í•‘', 'Checklist ìë™ ìƒì„±'])

    # file upload tab êµ¬í˜„
    with t1:
        uploaded_file = st.file_uploader("ìµœì¢… í†µí•© Worklistì˜ Excel or CSV íŒŒì¼ì„ ì—…ë¡œë“œ í•´ì£¼ì„¸ìš”.", type=["xlsx", "csv"])
        if uploaded_file is not None and st.button("Process File", key="worklist_sep_button1"):
             # ìƒˆ íŒŒì¼ ì—…ë¡œë“œ ì‹œ ê¸°ì¡´ ìƒíƒœ ì´ˆê¸°í™”
            st.session_state['checklist_state'] = {}
            
            with st.spinner('ë¶„ë¦¬ ì‘ì—… ì¤‘...'):
                processed_files = split_excel_sheets_to_files(uploaded_file)
                st.write('Worklistê°€ Sheetë³„ë¡œ ë¶„ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤!!')
                # Save processed files in session state
                st.session_state['checklist_state']['processed_files'] = processed_files
                st.session_state['checklist_state']['selected_files'] = []
                
        if 'processed_files' in st.session_state['checklist_state'] and st.session_state['checklist_state']['processed_files']:
            # File selection
            file_names = [file_name for file_name, _ in st.session_state['checklist_state']['processed_files']]
            selected_files = st.multiselect('ë‹¤ìš´ë¡œë“œí•  íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”:', file_names, key='file_selector')

            # Update selected files in session state
            st.session_state['checklist_state']['selected_files'] = selected_files

            # Download button
            if st.button('Download Selected Files', key="download_button1"):
                for file_name, file_data in st.session_state['checklist_state']['processed_files']:
                    if file_name in st.session_state['checklist_state']['selected_files']:
                        st.download_button(label=f"Download {file_name}", data=file_data, file_name=file_name, mime="application/vnd.ms-excel")
                        st.success('íŒŒì¼ì´ ì •ìƒì ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ ë˜ì—ˆìŠµë‹ˆë‹¤!!')
               
    with t2:
        # Place the file processing code inside this block
        uploaded_file = st.file_uploader("ë¶„ë¦¬ëœ Worklistì˜ Excel or CSV íŒŒì¼ì„ ì—…ë¡œë“œ í•´ì£¼ì„¸ìš”.", type=["csv", "xlsx"])
        file_name_input = st.text_input("ë‹¤ìš´ë¡œë“œ í•  íŒŒì¼ëª…ì„ ì •í•´ì£¼ì„¸ìš”. ex) ë³€ê²½ëœ 62 No.3 CDU Worklist.xlsx", "processed_file.xlsx")

        if st.button('Process File', key="worklist_trans_button1") and uploaded_file is not None:
            with st.spinner('ë³€ê²½ ì‘ì—… ì¤‘...'):
            
                # Perform the initial transformation
                temp_file_path = worklist_type_transform(uploaded_file)

                # Define a path for the file after inserting rows
                _, intermediate_file_path = tempfile.mkstemp(suffix='.xlsx')

                # Insert rows based on condition
                insert_row_based_on_condition(temp_file_path, intermediate_file_path)

                # Define a path for the final file
                _, final_file_path = tempfile.mkstemp(suffix='.xlsx')

                # Drop the first row in the final file
                drop_first_row(intermediate_file_path, final_file_path)
                
                st.write('Worklistì˜ í˜•íƒœê°€ ë¶„ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤!!')

                # Provide download link for the final file
                with open(final_file_path, 'rb') as file:
                    st.download_button(label="Download Files",
                                       data=file,
                                       file_name=file_name_input,
                                       mime="application/vnd.ms-excel")
                    st.write('íŒŒì¼ì´ ì •ìƒì ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ ë˜ì—ˆìŠµë‹ˆë‹¤!!')

                # Clean up temporary files
                os.remove(temp_file_path)
                os.remove(intermediate_file_path)
                os.remove(final_file_path)
        
    with t3:
        # íŒŒì¼ ì—…ë¡œë”
        uploaded_file = st.file_uploader("ë³€ê²½ëœ Worklistì˜ Excel or CSV íŒŒì¼ì„ ì—…ë¡œë“œ í•´ì£¼ì„¸ìš”.", type=["csv", "xlsx"])
        if st.button('Process File 1') and uploaded_file is not None:
            with st.spinner('ê·¸ë£¨í•‘ 1 ì‘ì—… ì¤‘...'):
                
                # íŒŒì¼ í˜•ì‹ ê²°ì •
                file_type = 'csv' if uploaded_file.name.endswith('.csv') else 'xlsx'
                # group_and_save_data í•¨ìˆ˜ ì‹¤í–‰
                grouped_file_path = group_and_save_data(uploaded_file, file_type)
                # Save to session state
                st.session_state['checklist_state']['grouped_file_path'] = grouped_file_path 
                st.session_state['checklist_state']['prepared_file_paths'] = {}
                st.write('ì¥ì¹˜ ë³„ Sheet ë¶„ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!!')
        else:
                st.error('ìœ íš¨í•œ íŒŒì¼ ê²½ë¡œê°€ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì„ ë‹¤ì‹œ ì—…ë¡œë“œí•˜ì„¸ìš”.')
        
        if st.session_state['checklist_state'].get('grouped_file_path'):
            with st.spinner('ê·¸ë£¨í•‘ 2 ì‘ì—… ì¤‘...'):
                file_paths = main_grouping(st.session_state['checklist_state']['grouped_file_path'])

                # Store prepared files in session state
                for category, file_path in file_paths.items():
                    st.session_state['checklist_state']['prepared_file_paths'][category] = file_path

                # Clean up temporary files and session state
                del st.session_state['checklist_state']['grouped_file_path']
        else:
                st.error('ìœ íš¨í•œ íŒŒì¼ ê²½ë¡œê°€ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì„ ë‹¤ì‹œ ì—…ë¡œë“œí•˜ì„¸ìš”.')
                
         # Create download buttons for prepared files
        for category, file_path in st.session_state['checklist_state'].get('prepared_file_paths', {}).items():
            with open(file_path, "rb") as file:
                st.download_button(label=f"Download {category} File", data=file, file_name=file_path, mime="application/vnd.ms-excel")
                
    with t4:
        # Create nested tabs within Main Tab 1
        nested_tab1, nested_tab2, nested_tab3 = st.tabs(["CDV Checklist", "HEX Checklist", 'AFC Checklist'])
        with nested_tab1:
            # íŒŒì¼ ì—…ë¡œë“œ ìœ„ì ¯
            uploaded_file = st.file_uploader("ê·¸ë£¨í•‘ëœ CDV Worklistë¥¼ ì—…ë¡œë“œ í•˜ì„¸ìš”.", type=["xlsx", "xls"])
            st.session_state['checklist_state']['filename1'] = st.text_input("ë‹¤ìš´ë¡œë“œ í•  í´ë”ëª…ì„ ì…ë ¥í•˜ì„¸ìš”. ex) 62ê³µì • CDV:", st.session_state['checklist_state']['filename1'])
            
            # "Generate" ë²„íŠ¼
            if st.button("Generate Checklist", key="generate_checklist_button1"):
                if uploaded_file is not None:
                    
                    zip_file = cdv_checklist(uploaded_file, st.session_state['checklist_state']['filename1'])

                    # ìƒì„±ëœ zip íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œ ë§í¬ë¡œ ì œê³µ
                    with open(zip_file, "rb") as f:
                        st.download_button(
                            label="Download Checklists",
                            data=f,
                            file_name="CDV Checklists.zip",
                            mime="application/zip"
                        )
                else:
                    st.error("Please upload a file.")
                    
        with nested_tab2:
            # íŒŒì¼ ì—…ë¡œë“œ ìœ„ì ¯
            uploaded_file = st.file_uploader("ê·¸ë£¨í•‘ëœ HEX Worklistë¥¼ ì—…ë¡œë“œ í•˜ì„¸ìš”.", type=["xlsx", "xls"])
            st.session_state['checklist_state']['filename1'] = st.text_input("ë‹¤ìš´ë¡œë“œ í•  í´ë”ëª…ì„ ì…ë ¥í•˜ì„¸ìš”. ex) 62ê³µì • HEX:", st.session_state['checklist_state']['filename1'])
            
            # "Generate" ë²„íŠ¼
            if st.button("Generate Checklist", key="generate_checklist_button2"):
                if uploaded_file is not None:
                    
                    zip_file = hex_checklist(uploaded_file, st.session_state['checklist_state']['filename1'])

                    # ìƒì„±ëœ zip íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œ ë§í¬ë¡œ ì œê³µ
                    with open(zip_file, "rb") as f:
                        st.download_button(
                            label="Download Checklists",
                            data=f,
                            file_name="HEX Checklists.zip",
                            mime="application/zip"
                        )
                else:
                    st.error("Please upload a file.")
                    
        with nested_tab3:
            # íŒŒì¼ ì—…ë¡œë“œ ìœ„ì ¯
            uploaded_file = st.file_uploader("ê·¸ë£¨í•‘ëœ AFC Worklistë¥¼ ì—…ë¡œë“œ í•˜ì„¸ìš”.", type=["xlsx", "xls"])
            st.session_state['checklist_state']['filename1'] = st.text_input("ë‹¤ìš´ë¡œë“œ í•  í´ë”ëª…ì„ ì…ë ¥í•˜ì„¸ìš”. ex) 62ê³µì • AFC:", st.session_state['checklist_state']['filename1'])
            
            # "Generate" ë²„íŠ¼
            if st.button("Generate Checklist", key="generate_checklist_button3"):
                if uploaded_file is not None:
                    
                    zip_file = afc_checklist(uploaded_file, st.session_state['checklist_state']['filename1'])

                    # ìƒì„±ëœ zip íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œ ë§í¬ë¡œ ì œê³µ
                    with open(zip_file, "rb") as f:
                        st.download_button(
                            label="Download Checklists",
                            data=f,
                            file_name="AFC Checklists.zip",
                            mime="application/zip"
                        )
                else:
                    st.error("Please upload a file.")

######################################################################################################
# handbook í˜ì´ì§€ ì¶œë ¥ í•¨ìˆ˜
def handbook_page():
    st.title('Handbook Generator')
    
    if 'handbook_state' not in st.session_state:
        st.session_state['handbook_state'] = {}
        
    # Initialize filename1 if not already done
    if 'title1' not in st.session_state:
        st.session_state['handbook_state']['title1'] = None

    # tabsë¥¼ ì¶”ê°€í•˜ì„¸ìš”.
    t1, t2 = st.tabs(['Checklist í†µí•©ë³¸ ìƒì„±', 'Handbook ìƒì„±'])

    # file upload tab êµ¬í˜„
    with t1:
        nested_tab1, nested_tab2, nested_tab3 = st.tabs(["CDV Checklist í†µí•©", "HEX Checklist í†µí•©", 'AFC Checklist í†µí•©'])
        with nested_tab1:
            uploaded_file = st.file_uploader("ê·¸ë£¨í•‘ëœ CDV Worklistì˜ Excel or CSV íŒŒì¼ì„ ì—…ë¡œë“œ í•´ì£¼ì„¸ìš”.", type=["xlsx", "csv"])
            if uploaded_file and st.button("Process File", key="worklist_merge_button1"):
                with st.spinner('í†µí•© ì‘ì—… ì¤‘...'):
                    saved_file_path = cdv_checklist_merge(uploaded_file)
                    st.write('Checklist í†µí•©ë³¸ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!!')

                    # Download button
                    with open(saved_file_path, "rb") as file:
                        btn = st.download_button(
                                label="Download Merged Checklist",
                                data=file,
                                file_name="CDV,HTR_Checklist_í†µí•©.xlsx",
                                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                            )
                    if btn:
                        st.success("íŒŒì¼ì´ ì •ìƒ ë‹¤ìš´ë¡œë“œ ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        with nested_tab2:
            uploaded_file = st.file_uploader("ê·¸ë£¨í•‘ëœ HEX Worklistì˜ Excel or CSV íŒŒì¼ì„ ì—…ë¡œë“œ í•´ì£¼ì„¸ìš”.", type=["xlsx", "csv"])
            if uploaded_file and st.button("Process File", key="worklist_merge_button2"):
                with st.spinner('í†µí•© ì‘ì—… ì¤‘...'):
                    saved_file_path = hex_checklist_merge(uploaded_file)
                    st.write('Checklist í†µí•©ë³¸ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!!')

                    # Download button
                    with open(saved_file_path, "rb") as file:
                        btn = st.download_button(
                                label="Download Merged Checklist",
                                data=file,
                                file_name="HEX_Checklist í†µí•©.xlsx",
                                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                            )
                    if btn:
                        st.success("íŒŒì¼ì´ ì •ìƒ ë‹¤ìš´ë¡œë“œ ë˜ì—ˆìŠµë‹ˆë‹¤.")
                        
        with nested_tab3:
            uploaded_file = st.file_uploader("ê·¸ë£¨í•‘ëœ AFC Worklistì˜ Excel or CSV íŒŒì¼ì„ ì—…ë¡œë“œ í•´ì£¼ì„¸ìš”.", type=["xlsx", "csv"])
            if uploaded_file and st.button("Process File", key="worklist_merge_button3"):
                with st.spinner('í†µí•© ì‘ì—… ì¤‘...'):
                    saved_file_path = afc_checklist_merge(uploaded_file)
                    st.write('Checklist í†µí•©ë³¸ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!!')

                    # Download button
                    with open(saved_file_path, "rb") as file:
                        btn = st.download_button(
                                label="Download Merged Checklist",
                                data=file,
                                file_name="AFC_Checklist í†µí•©.xlsx",
                                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                            )
                    if btn:
                        st.success("íŒŒì¼ì´ ì •ìƒ ë‹¤ìš´ë¡œë“œ ë˜ì—ˆìŠµë‹ˆë‹¤.")
                        
    with t2:
        nested_tab1, nested_tab2, nested_tab3 = st.tabs(["CDV Handbook ìƒì„±", "HEX / AFC Handbook ìƒì„±", 'Handbook í†µí•©'])
        with nested_tab1:
            uploaded_file = st.file_uploader("CDV Checklist í†µí•©íŒŒì¼ì„ ì—…ë¡œë“œ í•´ì£¼ì„¸ìš”.", type=["xlsx", "csv"])
            st.session_state['handbook_state']['title1'] = st.text_input('íŒŒì¼ì˜ titleì„ ì‘ì„±í•´ì£¼ì„¸ìš”. ex) 62ê³µì • Checklist:', st.session_state['handbook_state']['title1'])
            if st.button("Process File", key="handbook_generate_button1"):
                if uploaded_file is not None: 
                    with st.spinner('Handbook ìƒì„± ì¤‘...'):
                        saved_file_path = create_handbook_cdv(uploaded_file, st.session_state['handbook_state']['title1'])
                        st.write('CDV Handbookì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!!')

                        # Download button
                        with open(saved_file_path, "rb") as file:
                            btn = st.download_button(
                                    label="Download CDV Handbook",
                                    data=file,
                                    file_name="handbook.docx",
                                    mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
                                )
                        if btn:
                            st.success("íŒŒì¼ì´ ì •ìƒ ë‹¤ìš´ë¡œë“œ ë˜ì—ˆìŠµë‹ˆë‹¤.")
                            
        with nested_tab2:
            uploaded_file = st.file_uploader("HEX ë˜ëŠ” AFC Checklist í†µí•©íŒŒì¼ì„ ì—…ë¡œë“œ í•´ì£¼ì„¸ìš”.", type=["xlsx", "csv"])
            # st.session_state['handbook_state']['title1'] = st.text_input('íŒŒì¼ì˜ titleì„ ì‘ì„±í•´ì£¼ì„¸ìš”. ex) 62ê³µì • Checklist:', st.session_state['handbook_state']['title1'])
            if st.button("Process File", key="handbook_generate_button2"):
                if uploaded_file is not None: 
                    with st.spinner('Handbook ìƒì„± ì¤‘...'):
                        saved_file_path = create_handbook_hex_afc(uploaded_file)
                        st.write('HEX / AFC Handbookì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!!')

                        # Download button
                        with open(saved_file_path, "rb") as file:
                            btn = st.download_button(
                                    label="Download HEX/AFC Handbook",
                                    data=file,
                                    file_name="handbook.docx",
                                    mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
                                )
                        if btn:
                            st.success("íŒŒì¼ì´ ì •ìƒ ë‹¤ìš´ë¡œë“œ ë˜ì—ˆìŠµë‹ˆë‹¤.")
                            
        with nested_tab3:
            uploaded_files = st.file_uploader("í†µí•© í•˜ì‹¤ handbook íŒŒì¼ì„ ì—…ë¡œë“œ í•´ì£¼ì„¸ìš”.   â€» ì£¼ì˜ì‚¬í•­ : í†µí•© ì‹œí‚¬ íŒŒì¼ì„ ìˆœì„œëŒ€ë¡œ ì—…ë¡œë“œ í•´ì£¼ì„¸ìš”.", type=['docx'], accept_multiple_files=True)
            # st.session_state['handbook_state']['title1'] = st.text_input('íŒŒì¼ì˜ titleì„ ì‘ì„±í•´ì£¼ì„¸ìš”. ex) 62ê³µì • Checklist:', st.session_state['handbook_state']['title1'])
            if st.button("Integrate File", key="handbook_integrate_button1"):
                if uploaded_files is not None: 
                    with st.spinner('Handbook í†µí•© ì¤‘...'):
                        integrated_doc_path = integrate_docx_files(uploaded_files)
                        st.write('Handbookì´ í†µí•©ë˜ì—ˆìŠµë‹ˆë‹¤!!')

                        # Download button
                        with open(integrated_doc_path, "rb") as file:
                            btn = st.download_button(
                                    label="Download TA Handbook",
                                    data=file,
                                    file_name="handbook_integrated.docx",
                                    mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
                                )
                        if btn:
                            st.success("íŒŒì¼ì´ ì •ìƒ ë‹¤ìš´ë¡œë“œ ë˜ì—ˆìŠµë‹ˆë‹¤.")
                
######################################################################################################
                
# session_stateì— ì‚¬ì „ sidebar_state, eda_state, modeling_state, using_stateë¥¼ ì¶”ê°€í•˜ì„¸ìš”.
if 'sidebar_state' not in st.session_state:
    st.session_state['sidebar_state'] = {}
    st.session_state['sidebar_state']['current_page'] = front_page
if 'eda_state' not in st.session_state:
    st.session_state['eda_state'] = {}
if 'modeling_state' not in st.session_state:
    st.session_state['modeling_state'] = {}
if 'using_state' not in st.session_state:
    st.session_state['using_state'] = {}
if 'checklist_state' not in st.session_state:
    st.session_state['checklist_state'] = {}
if 'handbook_state' not in st.session_state:
    st.session_state['handbook_state'] = {}
    
# sidebar ì¶”ê°€
with st.sidebar:
    image = Image.open("./ì°¸ì¡°/ì¹¼í…ìŠ¤ ë¡œê³ .png")  # Replace with the path to your image
    st.image(image, use_column_width=True)
    st.subheader('Dashboard Menu')
    b1 = st.button('Front Page', use_container_width=True)
    b2 = st.button('EDA Page', use_container_width=True)
    b3 = st.button('Modeling Page', use_container_width=True)
    b4 = st.button('Using Page', use_container_width=True)
    st.divider()
    b5 = st.button('Checklist Page', use_container_width=True)
    b6 = st.button('Handbook Page', use_container_width=True)
    
if b1:
    st.session_state['sidebar_state']['current_page'] = front_page
#     st.session_state['sidebar_state']['current_page']()
    front_page()
elif b2:
    st.session_state['sidebar_state']['current_page'] = eda_page
#     st.session_state['sidebar_state']['current_page']()
    eda_page()
elif b3:
    st.session_state['sidebar_state']['current_page'] = modeling_page
#     st.session_state['sidebar_state']['current_page']()
    modeling_page()
elif b4:
    st.session_state['sidebar_state']['current_page'] = using_page
#     st.session_state['sidebar_state']['current_page']()
    using_page()
elif b5:
    st.session_state['sidebar_state']['current_page'] = checklist_page
#     st.session_state['sidebar_state']['current_page']()
    checklist_page()
elif b6:
    st.session_state['sidebar_state']['current_page'] = handbook_page
#     st.session_state['sidebar_state']['current_page']()
    handbook_page() 
else:
    st.session_state['sidebar_state']['current_page']()
